//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-24330188
// Cuda compilation tools, release 9.2, V9.2.148
// Based on LLVM 3.4svn
//

.version 6.2
.target sm_30
.address_size 64

	// .globl	_Z15DummyFastDivPTXPKyPKjPy
.const .align 4 .b8 RCP_C[1024] = {115, 190, 1, 254, 1, 255, 7, 253, 90, 140, 17, 250, 19, 251, 36, 249, 219, 205, 48, 246, 60, 247, 88, 245, 52, 41, 95, 242, 123, 243, 163, 241, 98, 69, 156, 238, 208, 239, 2, 238, 213, 206, 231, 234, 58, 236, 118, 234, 48, 115, 65, 231, 184, 232, 255, 230, 23, 226, 168, 227, 74, 229, 155, 227, 3, 205, 29, 224, 240, 225, 74, 224, 59, 234, 159, 220, 168, 222, 11, 221, 56, 239, 46, 217, 115, 219, 222, 217, 38, 150, 202, 213, 79, 216, 195, 214, 220, 153, 114, 210, 60, 213, 185, 211, 89, 182, 38, 207, 58, 210, 191, 208, 9, 171, 230, 203, 72, 207, 213, 205, 134, 56, 178, 200, 101, 204, 250, 202, 229, 32, 137, 197, 146, 201, 46, 200, 62, 40, 107, 194, 206, 198, 114, 197, 215, 19, 88, 191, 25, 196, 195, 194, 219, 172, 79, 188, 113, 193, 35, 192, 246, 185, 81, 185, 215, 190, 143, 189, 200, 5, 94, 182, 75, 188, 9, 187, 151, 93, 116, 179, 203, 185, 144, 184, 4, 141, 148, 176, 88, 183, 36, 182, 232, 97, 190, 173, 242, 180, 195, 179, 42, 174, 241, 170, 151, 178, 110, 177, 46, 65, 46, 168, 72, 176, 37, 175, 152, 236, 115, 165, 5, 174, 231, 172, 25, 133, 194, 162, 205, 171, 180, 170, 28, 223, 25, 160, 159, 169, 140, 168, 145, 207, 121, 157, 124, 167, 110, 166, 248, 45, 226, 154, 99, 165, 91, 164, 206, 208, 82, 152, 84, 163, 81, 162, 46, 145, 203, 149, 79, 161, 80, 160, 214, 72, 76, 147, 84, 159, 90, 158, 40, 210, 212, 144, 98, 157, 108, 156, 57, 9, 101, 142, 121, 155, 135, 154, 245, 202, 252, 139, 152, 153, 172, 152, 18, 242, 155, 137, 193, 151, 216, 150, 237, 94, 66, 135, 242, 149, 13, 149, 211, 239, 239, 132, 43, 148, 74, 147, 80, 132, 164, 130, 108, 146, 143, 145, 180, 252, 95, 128, 181, 144, 220, 143, 183, 58, 34, 126, 5, 143, 48, 142, 113, 31, 235, 123, 93, 141, 140, 140, 226, 140, 186, 121, 189, 139, 239, 138, 62, 104, 144, 119, 35, 138, 89, 137, 67, 147, 108, 117, 145, 136, 202, 135, 104, 244, 78, 115, 5, 135, 66, 134, 251, 110, 55, 113, 129, 133, 193, 132, 235, 233, 37, 111, 2, 132, 69, 131, 52, 75, 26, 109, 138, 130, 208, 129, 82, 122, 20, 107, 24, 129, 98, 128, 251, 92, 20, 105, 173, 127, 249, 126, 57, 221, 25, 103, 71, 126, 150, 125, 171, 226, 36, 101, 231, 124, 57, 124, 27, 86, 53, 99, 141, 123, 226, 122, 234, 33, 75, 97, 56, 122, 144, 121, 16, 47, 102, 95, 233, 120, 68, 120, 223, 103, 134, 93, 160, 119, 253, 118, 135, 184, 171, 91, 91, 118, 187, 117, 46, 11, 214, 89, 28, 117, 126, 116, 37, 77, 5, 88, 225, 115, 70, 115, 143, 104, 57, 86, 172, 114, 19, 114, 45, 76, 114, 84, 123, 113, 229, 112, 156, 226, 175, 82, 80, 112, 187, 111, 5, 28, 242, 80, 40, 111, 150, 110, 18, 228, 56, 79, 6, 110, 118, 109, 145, 42, 132, 77, 231, 108, 90, 108, 208, 220, 211, 75, 205, 107, 66, 107, 106, 233, 39, 74, 184, 106, 46, 106, 94, 65, 128, 72, 166, 105, 31, 105, 93, 210, 220, 70, 153, 104, 19, 104, 244, 141, 61, 69, 143, 103, 12, 103, 165, 98, 162, 67, 138, 102, 8, 102, 214, 66, 11, 66, 136, 101, 9, 101, 211, 29, 120, 64, 138, 100, 13, 100, 154, 228, 232, 62, 144, 99, 20, 99, 17, 138, 93, 61, 153, 98, 31, 98, 224, 254, 213, 59, 166, 97, 46, 97, 150, 52, 82, 58, 183, 96, 64, 96, 117, 30, 210, 56, 203, 95, 86, 95, 196, 174, 85, 55, 226, 94, 111, 94, 143, 215, 220, 53, 253, 93, 139, 93, 114, 141, 103, 52, 26, 93, 171, 92, 124, 193, 245, 50, 59, 92, 205, 91, 241, 103, 135, 49, 96, 91, 243, 90, 17, 117, 28, 48, 135, 90, 27, 90, 202, 220, 180, 46, 177, 89, 71, 89, 92, 147, 80, 45, 222, 88, 118, 88, 250, 139, 239, 43, 14, 88, 167, 87, 92, 188, 145, 42, 65, 87, 219, 86, 143, 25, 55, 41, 118, 86, 18, 86, 14, 151, 223, 39, 175, 85, 76, 85, 120, 43, 139, 38, 234, 84, 136, 84, 161, 203, 57, 37, 40, 84, 199, 83, 132, 109, 235, 35, 104, 83, 9, 83, 68, 6, 160, 34, 171, 82, 77, 82, 211, 140, 87, 33, 240, 81, 148, 81, 249, 245, 17, 32, 56, 81, 221, 80, 142, 56, 207, 30, 130, 80, 40, 80, 83, 75, 143, 29, 207, 79, 118, 79, 171, 36, 82, 28, 30, 79, 198, 78, 135, 187, 23, 27, 111, 78, 24, 78, 63, 7, 224, 25, 194, 77, 109, 77, 10, 254, 170, 24, 24, 77, 196, 76, 243, 150, 120, 23, 112, 76, 28, 76, 22, 203, 72, 22, 202, 75, 120, 75, 81, 144, 27, 21, 38, 75, 213, 74, 234, 222, 240, 19, 132, 74, 52, 74, 243, 174, 200, 18, 228, 73, 149, 73, 41, 248, 162, 17, 70, 73, 248, 72, 255, 177, 127, 16, 171, 72, 93, 72, 240, 213, 94, 15, 17, 72, 196, 71, 193, 91, 64, 14, 121, 71, 45, 71, 218, 59, 36, 13, 227, 70, 152, 70, 161, 111, 10, 12, 78, 70, 5, 70, 242, 237, 242, 10, 188, 69, 115, 69, 99, 177, 221, 9, 43, 69, 227, 68, 100, 178, 202, 8, 156, 68, 85, 68, 213, 233, 185, 7, 15, 68, 201, 67, 115, 81, 171, 6, 131, 67, 62, 67, 65, 225, 158, 5, 250, 66, 181, 66, 199, 148, 148, 4, 113, 66, 46, 66, 255, 98, 140, 3, 235, 65, 168, 65, 139, 71, 134, 2, 102, 65, 36, 65, 132, 59, 130, 1, 226, 64, 161, 64, 131, 56, 128, 0, 96, 64, 28, 64};
// _ZZ15DummyFastDivPTXPKyPKjPyE3RCP has been demoted
// _ZZ11FastDivTestPKyjPyE3RCP has been demoted

.visible .entry _Z15DummyFastDivPTXPKyPKjPy(
	.param .u64 _Z15DummyFastDivPTXPKyPKjPy_param_0,
	.param .u64 _Z15DummyFastDivPTXPKyPKjPy_param_1,
	.param .u64 _Z15DummyFastDivPTXPKyPKjPy_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<35>;
	// demoted variable
	.shared .align 4 .b8 _ZZ15DummyFastDivPTXPKyPKjPyE3RCP[1024];

	ld.param.u64 	%rd1, [_Z15DummyFastDivPTXPKyPKjPy_param_0];
	ld.param.u64 	%rd2, [_Z15DummyFastDivPTXPKyPKjPy_param_1];
	ld.param.u64 	%rd3, [_Z15DummyFastDivPTXPKyPKjPy_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ntid.x;
	setp.gt.s32	%p1, %r1, 255;
	@%p1 bra 	BB0_3;

	mov.u32 	%r52, %r1;

BB0_2:
	mul.wide.s32 	%rd4, %r52, 4;
	mov.u64 	%rd5, RCP_C;
	add.s64 	%rd6, %rd5, %rd4;
	ld.const.u32 	%r5, [%rd6];
	shl.b32 	%r6, %r52, 2;
	mov.u32 	%r7, _ZZ15DummyFastDivPTXPKyPKjPyE3RCP;
	add.s32 	%r8, %r7, %r6;
	st.shared.u32 	[%r8], %r5;
	add.s32 	%r52, %r2, %r52;
	setp.lt.s32	%p2, %r52, 256;
	@%p2 bra 	BB0_2;

BB0_3:
	bar.sync 	0;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r10, %r9, %r2, %r1;
	cvta.to.global.u64 	%rd7, %rd1;
	mul.wide.s32 	%rd8, %r10, 8;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u64 	%rd10, [%rd9];
	cvta.to.global.u64 	%rd11, %rd2;
	mul.wide.s32 	%rd12, %r10, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u32 	%r11, [%rd13];
	// inline asm
	// fast_div_v2 BEGIN
	// inline asm
	mov.b64	{%r12, %r13}, %rd10;
	and.b32  	%r14, %r11, 2130706432;
	bfe.u32 	%r15, %r11, 8, 16;
	add.s32 	%r16, %r15, -32768;
	shr.u32 	%r17, %r14, 21;
	mov.u32 	%r18, _ZZ15DummyFastDivPTXPKyPKjPyE3RCP;
	add.s32 	%r19, %r18, %r17;
	setp.gt.s32	%p3, %r16, 0;
	ld.shared.u32 	%r20, [%r19+4];
	shr.u32 	%r21, %r20, 16;
	selp.b32	%r22, %r21, %r20, %p3;
	and.b32  	%r23, %r22, 65535;
	mul24.lo.s32 	%r24, %r23, %r16;
	shr.s32 	%r25, %r24, 6;
	ld.shared.u32 	%r26, [%r19];
	sub.s32 	%r27, %r26, %r25;
	cvt.u64.u32	%rd14, %r11;
	mul.wide.u32 	%rd15, %r27, %r11;
	shl.b64 	%rd16, %rd14, 32;
	add.s64 	%rd17, %rd15, %rd16;
	setp.lt.u64	%p4, %rd17, %rd15;
	shr.u32 	%r28, %r11, 1;
	cvt.u64.u32	%rd18, %r28;
	setp.lt.u64	%p5, %rd18, %rd17;
	and.pred  	%p6, %p5, %p4;
	selp.b32	%r29, 0, %r27, %p6;
	sub.s64 	%rd19, %rd18, %rd17;
	mov.b64	{%r30, %r31}, %rd19;
	cvt.u32.u64	%r32, %rd19;
	mul.hi.u32 	%r33, %r32, %r27;
	cvt.u64.u32	%rd20, %r33;
	mul.wide.u32 	%rd21, %r31, %r27;
	add.s64 	%rd22, %rd19, %rd20;
	add.s64 	%rd23, %rd22, %rd21;
	mov.b64	{%r34, %r35}, %rd23;
	add.s32 	%r36, %r35, %r29;
	cvt.u32.u64	%r37, %rd10;
	mul.hi.u32 	%r38, %r37, %r36;
	cvt.u64.u32	%rd24, %r38;
	mul.wide.u32 	%rd25, %r36, %r13;
	add.s64 	%rd26, %rd25, %rd24;
	add.s64 	%rd27, %rd26, %rd10;
	setp.lt.u64	%p7, %rd27, %rd26;
	mov.b64	{%r39, %r40}, %rd27;
	selp.u32	%r41, 1, 0, %p7;
	mov.b64	%rd28, {%r40, %r41};
	mul.lo.s64 	%rd29, %rd28, %rd14;
	sub.s64 	%rd30, %rd10, %rd29;
	shr.u64 	%rd31, %rd30, 63;
	setp.ge.s64	%p8, %rd30, %rd14;
	selp.u32	%r42, 1, 0, %p8;
	shr.s64 	%rd32, %rd30, 63;
	cvt.u32.u64	%r43, %rd32;
	add.s32 	%r44, %r42, %r40;
	cvt.u32.u64	%r45, %rd30;
	setp.eq.s64	%p9, %rd31, 0;
	selp.b32	%r46, 0, %r11, %p9;
	neg.s32 	%r47, %r11;
	selp.b32	%r48, %r47, 0, %p8;
	add.s32 	%r49, %r48, %r45;
	add.s32 	%r50, %r49, %r46;
	add.s32 	%r51, %r44, %r43;
	// inline asm
	// fast_div_v2 END
	// inline asm
	cvta.to.global.u64 	%rd33, %rd3;
	add.s64 	%rd34, %rd33, %rd8;
	st.global.v2.u32 	[%rd34], {%r51, %r50};
	ret;
}

	// .globl	_Z16DummyFastSqrtPTXPKyPj
.visible .entry _Z16DummyFastSqrtPTXPKyPj(
	.param .u64 _Z16DummyFastSqrtPTXPKyPj_param_0,
	.param .u64 _Z16DummyFastSqrtPTXPKyPj_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd1, [_Z16DummyFastSqrtPTXPKyPj_param_0];
	ld.param.u64 	%rd2, [_Z16DummyFastSqrtPTXPKyPj_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r8, %r6, %r5, %r7;
	mul.wide.s32 	%rd5, %r8, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u64 	%rd7, [%rd6];
	// inline asm
	// fast_sqrt_v2 BEGIN
	// inline asm
	mov.b64	{%r9, %r10}, %rd7;
	shr.u32 	%r11, %r10, 9;
	add.s32 	%r1, %r11, 1602224128;
	// inline asm
	mov.b32 	%f1, %r1;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f4, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r2, %f2;
	// inline asm
	add.s32 	%r3, %r2, 268435456;
	// inline asm
	mov.b32 	%f7, %r3;
	// inline asm
	// inline asm
	mov.b32 	%r4, %f4;
	// inline asm
	add.s32 	%r12, %r4, -1325400064;
	mul.wide.u32 	%rd8, %r12, %r12;
	shl.b64 	%rd9, %rd8, 18;
	sub.s64 	%rd10, %rd7, %rd9;
	mov.b64	{%r13, %r14}, %rd10;
	cvt.rn.f32.s32	%f9, %r14;
	mul.f32 	%f10, %f7, %f9;
	shl.b32 	%r15, %r12, 10;
	cvt.rni.s32.f32	%r16, %f10;
	add.s32 	%r17, %r15, %r16;
	shr.u32 	%r18, %r17, 1;
	and.b32  	%r19, %r17, 1;
	cvt.u64.u32	%rd11, %r18;
	add.s32 	%r20, %r18, %r19;
	mul.wide.u32 	%rd12, %r20, %r18;
	cvt.u64.u32	%rd13, %r17;
	shl.b64 	%rd14, %rd13, 32;
	sub.s64 	%rd15, %rd14, %rd7;
	add.s64 	%rd16, %rd12, %rd15;
	cvt.u64.u32	%rd17, %r19;
	add.s64 	%rd18, %rd16, %rd17;
	setp.gt.s64	%p1, %rd18, 0;
	selp.b32	%r21, -1, 0, %p1;
	add.s32 	%r22, %r21, %r17;
	add.s64 	%rd19, %rd11, %rd16;
	add.s64 	%rd20, %rd19, 4294967296;
	shr.u64 	%rd21, %rd20, 63;
	cvt.u32.u64	%r23, %rd21;
	add.s32 	%r24, %r22, %r23;
	// inline asm
	// fast_sqrt_v2 END
	// inline asm
	mul.wide.s32 	%rd22, %r8, 4;
	add.s64 	%rd23, %rd3, %rd22;
	st.global.u32 	[%rd23], %r24;
	ret;
}

	// .globl	_Z11FastDivTestPKyjPy
.visible .entry _Z11FastDivTestPKyjPy(
	.param .u64 _Z11FastDivTestPKyjPy_param_0,
	.param .u32 _Z11FastDivTestPKyjPy_param_1,
	.param .u64 _Z11FastDivTestPKyjPy_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<66>;
	.reg .b64 	%rd<44>;
	// demoted variable
	.shared .align 4 .b8 _ZZ11FastDivTestPKyjPyE3RCP[1024];

	ld.param.u64 	%rd11, [_Z11FastDivTestPKyjPy_param_0];
	ld.param.u32 	%r5, [_Z11FastDivTestPKyjPy_param_1];
	ld.param.u64 	%rd12, [_Z11FastDivTestPKyjPy_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ntid.x;
	setp.gt.s32	%p1, %r1, 255;
	@%p1 bra 	BB2_3;

	mov.u32 	%r65, %r1;

BB2_2:
	mul.wide.s32 	%rd13, %r65, 4;
	mov.u64 	%rd14, RCP_C;
	add.s64 	%rd15, %rd14, %rd13;
	ld.const.u32 	%r6, [%rd15];
	shl.b32 	%r7, %r65, 2;
	mov.u32 	%r8, _ZZ11FastDivTestPKyjPyE3RCP;
	add.s32 	%r9, %r8, %r7;
	st.shared.u32 	[%r9], %r6;
	add.s32 	%r65, %r2, %r65;
	setp.lt.s32	%p2, %r65, 256;
	@%p2 bra 	BB2_2;

BB2_3:
	bar.sync 	0;
	mov.u32 	%r10, %ctaid.x;
	mad.lo.s32 	%r11, %r10, %r2, %r1;
	shr.s32 	%r12, %r11, 31;
	shr.u32 	%r13, %r12, 24;
	add.s32 	%r14, %r11, %r13;
	and.b32  	%r15, %r14, -256;
	sub.s32 	%r16, %r11, %r15;
	cvta.to.global.u64 	%rd16, %rd11;
	mul.wide.s32 	%rd17, %r16, 8;
	add.s64 	%rd18, %rd16, %rd17;
	shr.s32 	%r17, %r14, 8;
	add.s32 	%r18, %r17, %r5;
	setp.eq.s32	%p3, %r18, -2147483648;
	selp.b32	%r19, -2147483647, %r18, %p3;
	ld.global.u64 	%rd1, [%rd18];
	mov.b64	{%r20, %r21}, %rd1;
	and.b32  	%r22, %r19, 2130706432;
	bfe.u32 	%r23, %r19, 8, 16;
	add.s32 	%r24, %r23, -32768;
	shr.u32 	%r25, %r22, 21;
	mov.u32 	%r26, _ZZ11FastDivTestPKyjPyE3RCP;
	add.s32 	%r27, %r26, %r25;
	setp.gt.s32	%p4, %r24, 0;
	ld.shared.u32 	%r28, [%r27+4];
	shr.u32 	%r29, %r28, 16;
	selp.b32	%r30, %r29, %r28, %p4;
	and.b32  	%r31, %r30, 65535;
	mul24.lo.s32 	%r32, %r31, %r24;
	shr.s32 	%r33, %r32, 6;
	ld.shared.u32 	%r34, [%r27];
	sub.s32 	%r35, %r34, %r33;
	cvt.u64.u32	%rd2, %r19;
	mul.wide.u32 	%rd19, %r35, %r19;
	shl.b64 	%rd20, %rd2, 32;
	add.s64 	%rd21, %rd19, %rd20;
	setp.lt.u64	%p5, %rd21, %rd19;
	shr.u32 	%r36, %r19, 1;
	cvt.u64.u32	%rd22, %r36;
	setp.lt.u64	%p6, %rd22, %rd21;
	and.pred  	%p7, %p6, %p5;
	selp.b32	%r37, 0, %r35, %p7;
	sub.s64 	%rd23, %rd22, %rd21;
	mov.b64	{%r38, %r39}, %rd23;
	cvt.u32.u64	%r40, %rd23;
	mul.hi.u32 	%r41, %r40, %r35;
	cvt.u64.u32	%rd24, %r41;
	mul.wide.u32 	%rd25, %r39, %r35;
	add.s64 	%rd26, %rd23, %rd24;
	add.s64 	%rd27, %rd26, %rd25;
	mov.b64	{%r42, %r43}, %rd27;
	add.s32 	%r44, %r43, %r37;
	cvt.u32.u64	%r45, %rd1;
	mul.hi.u32 	%r46, %r45, %r44;
	cvt.u64.u32	%rd28, %r46;
	mul.wide.u32 	%rd29, %r44, %r21;
	add.s64 	%rd30, %rd29, %rd28;
	add.s64 	%rd31, %rd30, %rd1;
	setp.lt.u64	%p8, %rd31, %rd30;
	mov.b64	{%r47, %r48}, %rd31;
	selp.u32	%r49, 1, 0, %p8;
	mov.b64	%rd32, {%r48, %r49};
	mul.lo.s64 	%rd33, %rd32, %rd2;
	sub.s64 	%rd34, %rd1, %rd33;
	shr.u64 	%rd35, %rd34, 63;
	setp.ge.s64	%p9, %rd34, %rd2;
	selp.u32	%r50, 1, 0, %p9;
	shr.s64 	%rd36, %rd34, 63;
	cvt.u32.u64	%r51, %rd36;
	add.s32 	%r52, %r50, %r48;
	cvt.u32.u64	%r53, %rd34;
	setp.eq.s64	%p10, %rd35, 0;
	selp.b32	%r54, 0, %r19, %p10;
	neg.s32 	%r55, %r19;
	selp.b32	%r56, %r55, 0, %p9;
	add.s32 	%r57, %r56, %r53;
	add.s32 	%r58, %r57, %r54;
	add.s32 	%r59, %r52, %r51;
	mov.b64	%rd3, {%r59, %r58};
	and.b64  	%rd37, %rd1, -4294967296;
	setp.eq.s64	%p11, %rd37, 0;
	@%p11 bra 	BB2_5;

	div.u64 	%rd42, %rd1, %rd2;
	rem.u64 	%rd43, %rd1, %rd2;
	bra.uni 	BB2_6;

BB2_5:
	cvt.u32.u64	%r60, %rd2;
	div.u32 	%r62, %r45, %r60;
	rem.u32 	%r63, %r45, %r60;
	cvt.u64.u32	%rd42, %r62;
	cvt.u64.u32	%rd43, %r63;

BB2_6:
	and.b64  	%rd38, %rd42, 4294967295;
	shl.b64 	%rd39, %rd43, 32;
	or.b64  	%rd10, %rd38, %rd39;
	setp.eq.s64	%p12, %rd3, %rd10;
	@%p12 bra 	BB2_9;

	cvta.to.global.u64 	%rd40, %rd12;
	atom.global.add.u32 	%r64, [%rd40], 1;
	setp.ne.s32	%p13, %r64, 0;
	@%p13 bra 	BB2_9;

	st.global.u64 	[%rd40+8], %rd1;
	st.global.u64 	[%rd40+16], %rd2;
	st.global.u64 	[%rd40+24], %rd3;
	st.global.u64 	[%rd40+32], %rd10;

BB2_9:
	ret;
}

	// .globl	_Z12FastSqrtTestjPy
.visible .entry _Z12FastSqrtTestjPy(
	.param .u32 _Z12FastSqrtTestjPy_param_0,
	.param .u64 _Z12FastSqrtTestjPy_param_1
)
{
	.reg .pred 	%p<18>;
	.reg .f32 	%f<51>;
	.reg .b32 	%r<109>;
	.reg .b64 	%rd<99>;


	ld.param.u32 	%r3, [_Z12FastSqrtTestjPy_param_0];
	ld.param.u64 	%rd15, [_Z12FastSqrtTestjPy_param_1];
	cvta.to.global.u64 	%rd1, %rd15;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r4, %r5, %r3;
	mov.u32 	%r7, %tid.x;
	add.s32 	%r1, %r6, %r7;
	setp.gt.s32	%p1, %r1, 1779033702;
	@%p1 bra 	BB3_13;
	bra.uni 	BB3_1;

BB3_13:
	setp.ne.s32	%p14, %r1, 1779033703;
	@%p14 bra 	BB3_17;

	mov.u32 	%r92, 1610612735;
	// inline asm
	mov.b32 	%f41, %r92;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f42, %f41;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f44, %f41;
	// inline asm
	// inline asm
	mov.b32 	%r93, %f42;
	// inline asm
	add.s32 	%r94, %r93, 268435456;
	// inline asm
	mov.b32 	%f47, %r94;
	// inline asm
	// inline asm
	mov.b32 	%r95, %f44;
	// inline asm
	add.s32 	%r96, %r95, -1325400064;
	mul.wide.u32 	%rd80, %r96, %r96;
	shl.b64 	%rd81, %rd80, 18;
	not.b64 	%rd82, %rd81;
	mov.b64	{%r97, %r98}, %rd82;
	cvt.rn.f32.s32	%f49, %r98;
	mul.f32 	%f50, %f47, %f49;
	shl.b32 	%r99, %r96, 10;
	cvt.rni.s32.f32	%r100, %f50;
	add.s32 	%r101, %r99, %r100;
	shr.u32 	%r102, %r101, 1;
	and.b32  	%r103, %r101, 1;
	cvt.u64.u32	%rd83, %r102;
	add.s32 	%r104, %r102, %r103;
	mul.wide.u32 	%rd84, %r104, %r102;
	cvt.u64.u32	%rd85, %r101;
	shl.b64 	%rd86, %rd85, 32;
	add.s64 	%rd87, %rd86, %rd84;
	add.s64 	%rd88, %rd87, 1;
	cvt.u64.u32	%rd89, %r103;
	add.s64 	%rd90, %rd88, %rd89;
	setp.gt.s64	%p15, %rd90, 0;
	selp.b32	%r105, -1, 0, %p15;
	add.s32 	%r106, %r105, %r101;
	add.s64 	%rd91, %rd83, %rd88;
	add.s64 	%rd92, %rd91, 4294967296;
	shr.u64 	%rd93, %rd92, 63;
	cvt.u32.u64	%r107, %rd93;
	add.s32 	%r2, %r106, %r107;
	setp.eq.s32	%p16, %r2, -736899889;
	@%p16 bra 	BB3_17;

	atom.global.add.u32 	%r108, [%rd1], 1;
	setp.ne.s32	%p17, %r108, 0;
	@%p17 bra 	BB3_17;

	cvt.u64.u32	%rd95, %r2;
	mov.u64 	%rd96, 1779033703;
	st.global.u64 	[%rd1+8], %rd96;
	mov.u64 	%rd97, -1;
	st.global.u64 	[%rd1+16], %rd97;
	mov.u64 	%rd98, 3558067407;
	st.global.u64 	[%rd1+24], %rd98;
	st.global.u64 	[%rd1+32], %rd95;
	bra.uni 	BB3_17;

BB3_1:
	cvt.s64.s32	%rd2, %r1;
	add.s64 	%rd3, %rd2, 4294967296;
	mul.lo.s64 	%rd4, %rd3, %rd3;
	mov.b64	{%r12, %r13}, %rd4;
	shr.u32 	%r14, %r13, 9;
	add.s32 	%r8, %r14, 1602224128;
	// inline asm
	mov.b32 	%f1, %r8;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f4, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r9, %f2;
	// inline asm
	add.s32 	%r10, %r9, 268435456;
	// inline asm
	mov.b32 	%f7, %r10;
	// inline asm
	// inline asm
	mov.b32 	%r11, %f4;
	// inline asm
	add.s32 	%r15, %r11, -1325400064;
	mul.wide.u32 	%rd16, %r15, %r15;
	shl.b64 	%rd17, %rd16, 18;
	sub.s64 	%rd18, %rd4, %rd17;
	mov.b64	{%r16, %r17}, %rd18;
	cvt.rn.f32.s32	%f9, %r17;
	mul.f32 	%f10, %f7, %f9;
	shl.b32 	%r18, %r15, 10;
	cvt.rni.s32.f32	%r19, %f10;
	add.s32 	%r20, %r18, %r19;
	shr.u32 	%r21, %r20, 1;
	and.b32  	%r22, %r20, 1;
	cvt.u64.u32	%rd19, %r21;
	add.s32 	%r23, %r21, %r22;
	mul.wide.u32 	%rd20, %r23, %r21;
	cvt.u64.u32	%rd21, %r20;
	shl.b64 	%rd22, %rd21, 32;
	sub.s64 	%rd23, %rd22, %rd4;
	add.s64 	%rd24, %rd20, %rd23;
	cvt.u64.u32	%rd25, %r22;
	add.s64 	%rd26, %rd24, %rd25;
	setp.gt.s64	%p2, %rd26, 0;
	selp.b32	%r24, -1, 0, %p2;
	add.s32 	%r25, %r24, %r20;
	add.s64 	%rd27, %rd19, %rd24;
	add.s64 	%rd28, %rd27, 4294967296;
	shr.u64 	%rd29, %rd28, 63;
	cvt.u32.u64	%r26, %rd29;
	add.s32 	%r27, %r25, %r26;
	cvt.u64.u32	%rd5, %r27;
	shl.b64 	%rd6, %rd3, 1;
	add.s64 	%rd7, %rd6, -8589934592;
	setp.eq.s64	%p3, %rd5, %rd7;
	@%p3 bra 	BB3_4;

	atom.global.add.u32 	%r28, [%rd1], 1;
	setp.ne.s32	%p4, %r28, 0;
	@%p4 bra 	BB3_4;

	st.global.u64 	[%rd1+8], %rd2;
	st.global.u64 	[%rd1+16], %rd4;
	st.global.u64 	[%rd1+24], %rd7;
	st.global.u64 	[%rd1+32], %rd5;

BB3_4:
	add.s64 	%rd8, %rd4, %rd3;
	mov.b64	{%r33, %r34}, %rd8;
	shr.u32 	%r35, %r34, 9;
	add.s32 	%r29, %r35, 1602224128;
	// inline asm
	mov.b32 	%f11, %r29;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f12, %f11;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f14, %f11;
	// inline asm
	// inline asm
	mov.b32 	%r30, %f12;
	// inline asm
	add.s32 	%r31, %r30, 268435456;
	// inline asm
	mov.b32 	%f17, %r31;
	// inline asm
	// inline asm
	mov.b32 	%r32, %f14;
	// inline asm
	add.s32 	%r36, %r32, -1325400064;
	mul.wide.u32 	%rd31, %r36, %r36;
	shl.b64 	%rd32, %rd31, 18;
	sub.s64 	%rd33, %rd8, %rd32;
	mov.b64	{%r37, %r38}, %rd33;
	cvt.rn.f32.s32	%f19, %r38;
	mul.f32 	%f20, %f17, %f19;
	shl.b32 	%r39, %r36, 10;
	cvt.rni.s32.f32	%r40, %f20;
	add.s32 	%r41, %r39, %r40;
	shr.u32 	%r42, %r41, 1;
	and.b32  	%r43, %r41, 1;
	cvt.u64.u32	%rd34, %r42;
	add.s32 	%r44, %r42, %r43;
	mul.wide.u32 	%rd35, %r44, %r42;
	cvt.u64.u32	%rd36, %r41;
	shl.b64 	%rd37, %rd36, 32;
	sub.s64 	%rd38, %rd37, %rd8;
	add.s64 	%rd39, %rd35, %rd38;
	cvt.u64.u32	%rd40, %r43;
	add.s64 	%rd41, %rd39, %rd40;
	setp.gt.s64	%p5, %rd41, 0;
	selp.b32	%r45, -1, 0, %p5;
	add.s32 	%r46, %r45, %r41;
	add.s64 	%rd42, %rd34, %rd39;
	add.s64 	%rd43, %rd42, 4294967296;
	shr.u64 	%rd44, %rd43, 63;
	cvt.u32.u64	%r47, %rd44;
	add.s32 	%r48, %r46, %r47;
	cvt.u64.u32	%rd9, %r48;
	setp.eq.s64	%p6, %rd9, %rd7;
	@%p6 bra 	BB3_7;

	atom.global.add.u32 	%r49, [%rd1], 1;
	setp.ne.s32	%p7, %r49, 0;
	@%p7 bra 	BB3_7;

	st.global.u64 	[%rd1+8], %rd2;
	st.global.u64 	[%rd1+16], %rd8;
	st.global.u64 	[%rd1+24], %rd7;
	st.global.u64 	[%rd1+32], %rd9;

BB3_7:
	add.s64 	%rd10, %rd8, 1;
	mov.b64	{%r54, %r55}, %rd10;
	shr.u32 	%r56, %r55, 9;
	add.s32 	%r50, %r56, 1602224128;
	// inline asm
	mov.b32 	%f21, %r50;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f22, %f21;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f24, %f21;
	// inline asm
	// inline asm
	mov.b32 	%r51, %f22;
	// inline asm
	add.s32 	%r52, %r51, 268435456;
	// inline asm
	mov.b32 	%f27, %r52;
	// inline asm
	// inline asm
	mov.b32 	%r53, %f24;
	// inline asm
	add.s32 	%r57, %r53, -1325400064;
	mul.wide.u32 	%rd46, %r57, %r57;
	shl.b64 	%rd47, %rd46, 18;
	sub.s64 	%rd48, %rd10, %rd47;
	mov.b64	{%r58, %r59}, %rd48;
	cvt.rn.f32.s32	%f29, %r59;
	mul.f32 	%f30, %f27, %f29;
	shl.b32 	%r60, %r57, 10;
	cvt.rni.s32.f32	%r61, %f30;
	add.s32 	%r62, %r60, %r61;
	shr.u32 	%r63, %r62, 1;
	and.b32  	%r64, %r62, 1;
	cvt.u64.u32	%rd49, %r63;
	add.s32 	%r65, %r63, %r64;
	mul.wide.u32 	%rd50, %r65, %r63;
	cvt.u64.u32	%rd51, %r62;
	shl.b64 	%rd52, %rd51, 32;
	sub.s64 	%rd53, %rd52, %rd10;
	add.s64 	%rd54, %rd50, %rd53;
	cvt.u64.u32	%rd55, %r64;
	add.s64 	%rd56, %rd54, %rd55;
	setp.gt.s64	%p8, %rd56, 0;
	selp.b32	%r66, -1, 0, %p8;
	add.s32 	%r67, %r66, %r62;
	add.s64 	%rd57, %rd49, %rd54;
	add.s64 	%rd58, %rd57, 4294967296;
	shr.u64 	%rd59, %rd58, 63;
	cvt.u32.u64	%r68, %rd59;
	add.s32 	%r69, %r67, %r68;
	cvt.u64.u32	%rd60, %r69;
	add.s64 	%rd11, %rd6, -8589934591;
	setp.eq.s64	%p9, %rd60, %rd11;
	@%p9 bra 	BB3_10;

	atom.global.add.u32 	%r70, [%rd1], 1;
	setp.ne.s32	%p10, %r70, 0;
	@%p10 bra 	BB3_10;

	st.global.u64 	[%rd1+8], %rd2;
	st.global.u64 	[%rd1+16], %rd10;
	st.global.u64 	[%rd1+24], %rd11;
	st.global.u64 	[%rd1+32], %rd9;

BB3_10:
	add.s64 	%rd62, %rd2, 4294967297;
	mul.lo.s64 	%rd63, %rd62, %rd62;
	add.s64 	%rd12, %rd63, -1;
	mov.b64	{%r75, %r76}, %rd12;
	shr.u32 	%r77, %r76, 9;
	add.s32 	%r71, %r77, 1602224128;
	// inline asm
	mov.b32 	%f31, %r71;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f32, %f31;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f34, %f31;
	// inline asm
	// inline asm
	mov.b32 	%r72, %f32;
	// inline asm
	add.s32 	%r73, %r72, 268435456;
	// inline asm
	mov.b32 	%f37, %r73;
	// inline asm
	// inline asm
	mov.b32 	%r74, %f34;
	// inline asm
	add.s32 	%r78, %r74, -1325400064;
	mul.wide.u32 	%rd64, %r78, %r78;
	shl.b64 	%rd65, %rd64, 18;
	sub.s64 	%rd66, %rd12, %rd65;
	mov.b64	{%r79, %r80}, %rd66;
	cvt.rn.f32.s32	%f39, %r80;
	mul.f32 	%f40, %f37, %f39;
	shl.b32 	%r81, %r78, 10;
	cvt.rni.s32.f32	%r82, %f40;
	add.s32 	%r83, %r81, %r82;
	shr.u32 	%r84, %r83, 1;
	and.b32  	%r85, %r83, 1;
	cvt.u64.u32	%rd67, %r84;
	add.s32 	%r86, %r84, %r85;
	mul.wide.u32 	%rd68, %r86, %r84;
	cvt.u64.u32	%rd69, %r83;
	shl.b64 	%rd70, %rd69, 32;
	sub.s64 	%rd71, %rd70, %rd12;
	add.s64 	%rd72, %rd68, %rd71;
	cvt.u64.u32	%rd73, %r85;
	add.s64 	%rd74, %rd72, %rd73;
	setp.gt.s64	%p11, %rd74, 0;
	selp.b32	%r87, -1, 0, %p11;
	add.s32 	%r88, %r87, %r83;
	add.s64 	%rd75, %rd67, %rd72;
	add.s64 	%rd76, %rd75, 4294967296;
	shr.u64 	%rd77, %rd76, 63;
	cvt.u32.u64	%r89, %rd77;
	add.s32 	%r90, %r88, %r89;
	cvt.u64.u32	%rd13, %r90;
	shl.b64 	%rd78, %rd62, 1;
	add.s64 	%rd14, %rd78, -8589934593;
	setp.eq.s64	%p12, %rd13, %rd14;
	@%p12 bra 	BB3_17;

	atom.global.add.u32 	%r91, [%rd1], 1;
	setp.ne.s32	%p13, %r91, 0;
	@%p13 bra 	BB3_17;

	st.global.u64 	[%rd1+8], %rd2;
	st.global.u64 	[%rd1+16], %rd12;
	st.global.u64 	[%rd1+24], %rd14;
	st.global.u64 	[%rd1+32], %rd13;

BB3_17:
	ret;
}


