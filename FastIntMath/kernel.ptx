//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-24330188
// Cuda compilation tools, release 9.2, V9.2.148
// Based on LLVM 3.4svn
//

.version 6.2
.target sm_30
.address_size 64

	// .globl	_Z20DummyFastDivHeavyPTXPKxPKiPx

.visible .entry _Z20DummyFastDivHeavyPTXPKxPKiPx(
	.param .u64 _Z20DummyFastDivHeavyPTXPKxPKiPx_param_0,
	.param .u64 _Z20DummyFastDivHeavyPTXPKxPKiPx_param_1,
	.param .u64 _Z20DummyFastDivHeavyPTXPKxPKiPx_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd1, [_Z20DummyFastDivHeavyPTXPKxPKiPx_param_0];
	ld.param.u64 	%rd2, [_Z20DummyFastDivHeavyPTXPKxPKiPx_param_1];
	ld.param.u64 	%rd3, [_Z20DummyFastDivHeavyPTXPKxPKiPx_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r6, %r4, %r3, %r5;
	mul.wide.s32 	%rd7, %r6, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd9, [%rd8];
	mul.wide.s32 	%rd10, %r6, 4;
	add.s64 	%rd11, %rd5, %rd10;
	ld.global.u32 	%r7, [%rd11];
	// inline asm
	// fast_div_heavy BEGIN
	// inline asm
	mov.b64	{%r8, %r9}, %rd9;
	abs.s64 	%rd12, %rd9;
	mov.b64	{%r10, %r11}, %rd12;
	abs.s32 	%r12, %r7;
	cvt.rn.f32.s32	%f2, %r12;
	// inline asm
	rcp.approx.f32 %f1, %f2;
	// inline asm
	// inline asm
	mov.b32 	%r1, %f1;
	// inline asm
	add.s32 	%r2, %r1, 268435456;
	// inline asm
	mov.b32 	%f4, %r2;
	// inline asm
	cvt.rn.f32.s32	%f5, %r11;
	mul.f32 	%f6, %f4, %f5;
	cvt.rni.u64.f32	%rd13, %f6;
	cvt.u64.u32	%rd14, %r12;
	mul.lo.s64 	%rd15, %rd13, %rd14;
	sub.s64 	%rd16, %rd12, %rd15;
	mov.b64	{%r13, %r14}, %rd16;
	cvt.rm.f32.s64	%f7, %rd16;
	mul.f32 	%f8, %f1, %f7;
	cvt.rni.s64.f32	%rd17, %f8;
	mov.b64	{%r15, %r16}, %rd17;
	mul.lo.s32 	%r17, %r15, %r12;
	sub.s32 	%r18, %r13, %r17;
	cvt.rn.f32.s32	%f9, %r18;
	mul.f32 	%f10, %f1, %f9;
	cvt.rni.s32.f32	%r19, %f10;
	mul.lo.s32 	%r20, %r19, %r12;
	sub.s32 	%r21, %r18, %r20;
	shr.s32 	%r22, %r21, 31;
	add.s32 	%r23, %r22, %r19;
	add.s64 	%rd18, %rd17, %rd13;
	cvt.s64.s32	%rd19, %r23;
	add.s64 	%rd20, %rd18, %rd19;
	xor.b32  	%r24, %r9, %r7;
	setp.lt.s32	%p1, %r24, 0;
	neg.s64 	%rd21, %rd20;
	selp.b64	%rd22, %rd21, %rd20, %p1;
	// inline asm
	// fast_div_heavy END
	// inline asm
	add.s64 	%rd23, %rd4, %rd7;
	st.global.u64 	[%rd23], %rd22;
	ret;
}

	// .globl	_Z15DummyFastDivPTXPKyPKjPy
.visible .entry _Z15DummyFastDivPTXPKyPKjPy(
	.param .u64 _Z15DummyFastDivPTXPKyPKjPy_param_0,
	.param .u64 _Z15DummyFastDivPTXPKyPKjPy_param_1,
	.param .u64 _Z15DummyFastDivPTXPKyPKjPy_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd1, [_Z15DummyFastDivPTXPKyPKjPy_param_0];
	ld.param.u64 	%rd2, [_Z15DummyFastDivPTXPKyPKjPy_param_1];
	ld.param.u64 	%rd3, [_Z15DummyFastDivPTXPKyPKjPy_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r7, %r5, %r4, %r6;
	mul.wide.s32 	%rd7, %r7, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd9, [%rd8];
	mul.wide.s32 	%rd10, %r7, 4;
	add.s64 	%rd11, %rd5, %rd10;
	ld.global.u32 	%r8, [%rd11];
	// inline asm
	// fast_div_v2 BEGIN
	// inline asm
	mov.b64	{%r9, %r10}, %rd9;
	shr.u32 	%r11, %r8, 8;
	add.s32 	%r1, %r11, 1317011456;
	// inline asm
	mov.b32 	%f1, %r1;
	// inline asm
	and.b32  	%r12, %r8, 255;
	cvt.rn.f32.u32	%f6, %r12;
	// inline asm
	rcp.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r2, %f2;
	// inline asm
	add.s32 	%r3, %r2, 536870912;
	// inline asm
	mov.b32 	%f5, %r3;
	// inline asm
	mov.f32 	%f7, 0fBF800000;
	fma.rn.f32 	%f8, %f1, %f2, %f7;
	fma.rn.f32 	%f9, %f6, %f2, %f8;
	shl.b32 	%r13, %r2, 9;
	mul.f32 	%f10, %f5, %f9;
	cvt.rni.s32.f32	%r14, %f10;
	sub.s32 	%r15, %r13, %r14;
	mul.hi.u32 	%r16, %r9, %r15;
	cvt.u64.u32	%rd12, %r16;
	mul.wide.u32 	%rd13, %r15, %r10;
	add.s64 	%rd14, %rd13, %rd12;
	add.s64 	%rd15, %rd14, %rd9;
	setp.lt.u64	%p1, %rd15, %rd14;
	mov.b64	{%r17, %r18}, %rd15;
	cvt.u64.u32	%rd16, %r8;
	mul.wide.u32 	%rd17, %r18, %r8;
	sub.s64 	%rd18, %rd9, %rd17;
	mov.b64	{%r19, %r20}, %rd18;
	selp.b32	%r21, %r8, 0, %p1;
	sub.s32 	%r22, %r20, %r21;
	mov.b64	%rd19, {%r19, %r22};
	setp.ge.s64	%p2, %rd19, %rd16;
	selp.u32	%r23, 1, 0, %p2;
	shr.s32 	%r24, %r22, 31;
	add.s32 	%r25, %r24, %r18;
	setp.gt.s32	%p3, %r22, -1;
	selp.b32	%r26, 0, %r8, %p3;
	add.s32 	%r27, %r26, %r19;
	neg.s32 	%r28, %r8;
	selp.b32	%r29, %r28, 0, %p2;
	add.s32 	%r30, %r27, %r29;
	add.s32 	%r31, %r25, %r23;
	// inline asm
	// fast_div_v2 END
	// inline asm
	add.s64 	%rd20, %rd4, %rd7;
	st.global.v2.u32 	[%rd20], {%r31, %r30};
	ret;
}

	// .globl	_Z16DummyFastSqrtPTXPKyPj
.visible .entry _Z16DummyFastSqrtPTXPKyPj(
	.param .u64 _Z16DummyFastSqrtPTXPKyPj_param_0,
	.param .u64 _Z16DummyFastSqrtPTXPKyPj_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd1, [_Z16DummyFastSqrtPTXPKyPj_param_0];
	ld.param.u64 	%rd2, [_Z16DummyFastSqrtPTXPKyPj_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r8, %r6, %r5, %r7;
	mul.wide.s32 	%rd5, %r8, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u64 	%rd7, [%rd6];
	// inline asm
	// fast_sqrt_v2 BEGIN
	// inline asm
	mov.b64	{%r9, %r10}, %rd7;
	shr.u32 	%r11, %r10, 9;
	add.s32 	%r1, %r11, 1602224128;
	// inline asm
	mov.b32 	%f1, %r1;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f4, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r2, %f2;
	// inline asm
	add.s32 	%r3, %r2, 268435456;
	// inline asm
	mov.b32 	%f7, %r3;
	// inline asm
	// inline asm
	mov.b32 	%r4, %f4;
	// inline asm
	add.s32 	%r12, %r4, -1325400064;
	mul.wide.u32 	%rd8, %r12, %r12;
	shl.b64 	%rd9, %rd8, 18;
	sub.s64 	%rd10, %rd7, %rd9;
	mov.b64	{%r13, %r14}, %rd10;
	cvt.rn.f32.s32	%f9, %r14;
	mul.f32 	%f10, %f7, %f9;
	shl.b32 	%r15, %r12, 10;
	cvt.rni.s32.f32	%r16, %f10;
	add.s32 	%r17, %r15, %r16;
	shr.u32 	%r18, %r17, 1;
	and.b32  	%r19, %r17, 1;
	cvt.u64.u32	%rd11, %r18;
	add.s32 	%r20, %r18, %r19;
	mul.wide.u32 	%rd12, %r20, %r18;
	cvt.u64.u32	%rd13, %r17;
	shl.b64 	%rd14, %rd13, 32;
	sub.s64 	%rd15, %rd14, %rd7;
	add.s64 	%rd16, %rd12, %rd15;
	cvt.u64.u32	%rd17, %r19;
	add.s64 	%rd18, %rd16, %rd17;
	setp.gt.s64	%p1, %rd18, 0;
	selp.b32	%r21, -1, 0, %p1;
	add.s32 	%r22, %r21, %r17;
	add.s64 	%rd19, %rd11, %rd16;
	add.s64 	%rd20, %rd19, 4294967296;
	shr.u64 	%rd21, %rd20, 63;
	cvt.u32.u64	%r23, %rd21;
	add.s32 	%r24, %r22, %r23;
	// inline asm
	// fast_sqrt_v2 END
	// inline asm
	mul.wide.s32 	%rd22, %r8, 4;
	add.s64 	%rd23, %rd3, %rd22;
	st.global.u32 	[%rd23], %r24;
	ret;
}

	// .globl	_Z17GetReciprocalTestjPy
.visible .entry _Z17GetReciprocalTestjPy(
	.param .u32 _Z17GetReciprocalTestjPy_param_0,
	.param .u64 _Z17GetReciprocalTestjPy_param_1
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<19>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<23>;


	ld.param.u32 	%r7, [_Z17GetReciprocalTestjPy_param_0];
	ld.param.u64 	%rd9, [_Z17GetReciprocalTestjPy_param_1];
	mov.u32 	%r8, %ctaid.x;
	add.s32 	%r9, %r8, %r7;
	setp.eq.s32	%p1, %r9, 0;
	selp.b32	%r10, 1, %r9, %p1;
	// inline asm
	// get_reciprocal64 BEGIN
	// inline asm
	clz.b32 	%r11, %r10;
	shl.b32 	%r12, %r10, %r11;
	shr.u32 	%r13, %r12, 8;
	add.s32 	%r1, %r13, 1317011457;
	// inline asm
	mov.b32 	%f1, %r1;
	// inline asm
	and.b32  	%r14, %r12, 255;
	add.s32 	%r15, %r14, -256;
	cvt.rn.f32.s32	%f9, %r15;
	// inline asm
	rcp.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r2, %f2;
	// inline asm
	shl.b32 	%r16, %r11, 23;
	add.s32 	%r17, %r16, %r2;
	add.s32 	%r3, %r17, 553648128;
	// inline asm
	mov.b32 	%f5, %r3;
	// inline asm
	mov.f32 	%f10, 0fBF800000;
	fma.rn.f32 	%f11, %f1, %f2, %f10;
	fma.rn.f32 	%f7, %f9, %f2, %f11;
	and.b32  	%r4, %r3, -4096;
	// inline asm
	mov.b32 	%f6, %r4;
	// inline asm
	// inline asm
	mov.b32 	%r5, %f7;
	// inline asm
	and.b32  	%r6, %r5, -4096;
	// inline asm
	mov.b32 	%f8, %r6;
	// inline asm
	sub.f32 	%f12, %f5, %f6;
	sub.f32 	%f13, %f7, %f8;
	mul.f32 	%f14, %f6, %f8;
	mul.f32 	%f15, %f12, %f8;
	fma.rn.f32 	%f16, %f13, %f5, %f15;
	cvt.rni.s64.f32	%rd10, %f14;
	cvt.rpi.s32.f32	%r18, %f16;
	add.f32 	%f17, %f14, %f16;
	mul.f32 	%f18, %f7, %f17;
	cvt.rmi.s32.f32	%r19, %f18;
	sub.s32 	%r20, %r18, %r19;
	and.b32  	%r21, %r2, 16777215;
	cvt.u64.u32	%rd11, %r21;
	add.s32 	%r22, %r11, 9;
	shl.b64 	%rd12, %rd11, %r22;
	cvt.s64.s32	%rd13, %r20;
	add.s64 	%rd14, %rd13, %rd10;
	shr.s64 	%rd15, %rd14, 2;
	sub.s64 	%rd1, %rd12, %rd15;
	// inline asm
	// get_reciprocal64 END
	// inline asm
	cvt.u64.u32	%rd2, %r10;
	mov.pred 	%p2, 0;
	@%p2 bra 	BB3_2;

	mov.u64 	%rd16, -1;
	div.u64 	%rd21, %rd16, %rd2;
	rem.u64 	%rd22, %rd16, %rd2;
	bra.uni 	BB3_3;

BB3_2:
	cvt.u32.u64	%r23, %rd2;
	mov.u32 	%r24, -1;
	div.u32 	%r25, %r24, %r23;
	rem.u32 	%r26, %r24, %r23;
	cvt.u64.u32	%rd21, %r25;
	cvt.u64.u32	%rd22, %r26;

BB3_3:
	add.s64 	%rd17, %rd22, 1;
	cvt.rn.f64.u64	%fd2, %rd17;
	cvt.u32.u64	%r27, %rd2;
	cvt.rn.f64.u32	%fd3, %r27;
	div.rn.f64 	%fd4, %fd2, %fd3;
	sub.s64 	%rd18, %rd1, %rd21;
	cvt.rn.f64.s64	%fd5, %rd18;
	sub.f64 	%fd1, %fd5, %fd4;
	setp.ge.f64	%p3, %fd1, 0d3FEC083126E978D5;
	setp.le.f64	%p4, %fd1, 0dBFEC083126E978D5;
	or.pred  	%p5, %p4, %p3;
	@!%p5 bra 	BB3_6;
	bra.uni 	BB3_4;

BB3_4:
	cvta.to.global.u64 	%rd19, %rd9;
	atom.global.add.u32 	%r28, [%rd19], 1;
	setp.ne.s32	%p6, %r28, 0;
	@%p6 bra 	BB3_6;

	st.global.u64 	[%rd19+8], %rd2;
	st.global.u64 	[%rd19+16], %rd1;
	st.global.u64 	[%rd19+24], %rd21;
	st.global.f64 	[%rd19+32], %fd1;

BB3_6:
	ret;
}

	// .globl	_Z11FastDivTestPKyjPy
.visible .entry _Z11FastDivTestPKyjPy(
	.param .u64 _Z11FastDivTestPKyjPy_param_0,
	.param .u32 _Z11FastDivTestPKyjPy_param_1,
	.param .u64 _Z11FastDivTestPKyjPy_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<30>;


	ld.param.u64 	%rd12, [_Z11FastDivTestPKyjPy_param_0];
	ld.param.u32 	%r4, [_Z11FastDivTestPKyjPy_param_1];
	ld.param.u64 	%rd11, [_Z11FastDivTestPKyjPy_param_2];
	mov.u32 	%r5, %tid.x;
	cvta.to.global.u64 	%rd13, %rd12;
	mul.wide.u32 	%rd14, %r5, 8;
	add.s64 	%rd15, %rd13, %rd14;
	mov.u32 	%r6, %ctaid.x;
	add.s32 	%r7, %r6, %r4;
	setp.eq.s32	%p1, %r7, -2147483648;
	selp.b32	%r8, -2147483647, %r7, %p1;
	ld.global.u64 	%rd1, [%rd15];
	mov.b64	{%r9, %r10}, %rd1;
	shr.u32 	%r11, %r8, 8;
	add.s32 	%r1, %r11, 1317011456;
	// inline asm
	mov.b32 	%f1, %r1;
	// inline asm
	and.b32  	%r12, %r8, 255;
	cvt.rn.f32.u32	%f6, %r12;
	// inline asm
	rcp.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r2, %f2;
	// inline asm
	add.s32 	%r3, %r2, 536870912;
	// inline asm
	mov.b32 	%f5, %r3;
	// inline asm
	mov.f32 	%f7, 0fBF800000;
	fma.rn.f32 	%f8, %f1, %f2, %f7;
	fma.rn.f32 	%f9, %f6, %f2, %f8;
	shl.b32 	%r13, %r2, 9;
	mul.f32 	%f10, %f5, %f9;
	cvt.rni.s32.f32	%r14, %f10;
	sub.s32 	%r15, %r13, %r14;
	mul.hi.u32 	%r16, %r9, %r15;
	cvt.u64.u32	%rd16, %r16;
	mul.wide.u32 	%rd17, %r15, %r10;
	add.s64 	%rd18, %rd17, %rd16;
	add.s64 	%rd19, %rd18, %rd1;
	setp.lt.u64	%p2, %rd19, %rd18;
	mov.b64	{%r17, %r18}, %rd19;
	cvt.u64.u32	%rd2, %r8;
	mul.wide.u32 	%rd20, %r18, %r8;
	sub.s64 	%rd21, %rd1, %rd20;
	mov.b64	{%r19, %r20}, %rd21;
	selp.b32	%r21, %r8, 0, %p2;
	sub.s32 	%r22, %r20, %r21;
	mov.b64	%rd22, {%r19, %r22};
	setp.ge.s64	%p3, %rd22, %rd2;
	selp.u32	%r23, 1, 0, %p3;
	shr.s32 	%r24, %r22, 31;
	add.s32 	%r25, %r24, %r18;
	setp.gt.s32	%p4, %r22, -1;
	selp.b32	%r26, 0, %r8, %p4;
	add.s32 	%r27, %r26, %r19;
	neg.s32 	%r28, %r8;
	selp.b32	%r29, %r28, 0, %p3;
	add.s32 	%r30, %r27, %r29;
	add.s32 	%r31, %r25, %r23;
	mov.b64	%rd3, {%r31, %r30};
	and.b64  	%rd23, %rd1, -4294967296;
	setp.eq.s64	%p5, %rd23, 0;
	@%p5 bra 	BB4_2;

	div.u64 	%rd28, %rd1, %rd2;
	rem.u64 	%rd29, %rd1, %rd2;
	bra.uni 	BB4_3;

BB4_2:
	cvt.u32.u64	%r32, %rd2;
	cvt.u32.u64	%r33, %rd1;
	div.u32 	%r34, %r33, %r32;
	rem.u32 	%r35, %r33, %r32;
	cvt.u64.u32	%rd28, %r34;
	cvt.u64.u32	%rd29, %r35;

BB4_3:
	and.b64  	%rd24, %rd28, 4294967295;
	shl.b64 	%rd25, %rd29, 32;
	or.b64  	%rd10, %rd24, %rd25;
	setp.eq.s64	%p6, %rd3, %rd10;
	@%p6 bra 	BB4_6;

	cvta.to.global.u64 	%rd26, %rd11;
	atom.global.add.u32 	%r36, [%rd26], 1;
	setp.ne.s32	%p7, %r36, 0;
	@%p7 bra 	BB4_6;

	st.global.u64 	[%rd26+8], %rd1;
	st.global.u64 	[%rd26+16], %rd2;
	st.global.u64 	[%rd26+24], %rd3;
	st.global.u64 	[%rd26+32], %rd10;

BB4_6:
	ret;
}

	// .globl	_Z16FastDivHeavyTestPKxiPx
.visible .entry _Z16FastDivHeavyTestPKxiPx(
	.param .u64 _Z16FastDivHeavyTestPKxiPx_param_0,
	.param .u32 _Z16FastDivHeavyTestPKxiPx_param_1,
	.param .u64 _Z16FastDivHeavyTestPKxiPx_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<27>;


	ld.param.u64 	%rd8, [_Z16FastDivHeavyTestPKxiPx_param_0];
	ld.param.u32 	%r3, [_Z16FastDivHeavyTestPKxiPx_param_1];
	ld.param.u64 	%rd7, [_Z16FastDivHeavyTestPKxiPx_param_2];
	mov.u32 	%r4, %tid.x;
	cvta.to.global.u64 	%rd9, %rd8;
	mul.wide.u32 	%rd10, %r4, 8;
	add.s64 	%rd11, %rd9, %rd10;
	mov.u32 	%r5, %ctaid.x;
	add.s32 	%r6, %r5, %r3;
	mov.u32 	%r7, 1;
	max.s32 	%r8, %r7, %r6;
	ld.global.u64 	%rd1, [%rd11];
	mov.b64	{%r9, %r10}, %rd1;
	abs.s64 	%rd12, %rd1;
	mov.b64	{%r11, %r12}, %rd12;
	abs.s32 	%r13, %r8;
	cvt.rn.f32.s32	%f2, %r13;
	// inline asm
	rcp.approx.f32 %f1, %f2;
	// inline asm
	// inline asm
	mov.b32 	%r1, %f1;
	// inline asm
	add.s32 	%r2, %r1, 268435456;
	// inline asm
	mov.b32 	%f4, %r2;
	// inline asm
	cvt.rn.f32.s32	%f5, %r12;
	mul.f32 	%f6, %f4, %f5;
	cvt.rni.u64.f32	%rd13, %f6;
	cvt.u64.u32	%rd14, %r13;
	mul.lo.s64 	%rd15, %rd13, %rd14;
	sub.s64 	%rd16, %rd12, %rd15;
	mov.b64	{%r14, %r15}, %rd16;
	cvt.rm.f32.s64	%f7, %rd16;
	mul.f32 	%f8, %f1, %f7;
	cvt.rni.s64.f32	%rd17, %f8;
	mov.b64	{%r16, %r17}, %rd17;
	mul.lo.s32 	%r18, %r16, %r13;
	sub.s32 	%r19, %r14, %r18;
	cvt.rn.f32.s32	%f9, %r19;
	mul.f32 	%f10, %f1, %f9;
	cvt.rni.s32.f32	%r20, %f10;
	mul.lo.s32 	%r21, %r20, %r13;
	sub.s32 	%r22, %r19, %r21;
	shr.s32 	%r23, %r22, 31;
	add.s32 	%r24, %r23, %r20;
	add.s64 	%rd18, %rd17, %rd13;
	cvt.s64.s32	%rd19, %r24;
	add.s64 	%rd20, %rd18, %rd19;
	xor.b32  	%r25, %r8, %r10;
	setp.lt.s32	%p1, %r25, 0;
	neg.s64 	%rd21, %rd20;
	selp.b64	%rd2, %rd21, %rd20, %p1;
	cvt.s64.s32	%rd3, %r8;
	or.b64  	%rd22, %rd1, %rd3;
	and.b64  	%rd23, %rd22, -4294967296;
	setp.eq.s64	%p2, %rd23, 0;
	@%p2 bra 	BB5_2;

	div.s64 	%rd26, %rd1, %rd3;
	bra.uni 	BB5_3;

BB5_2:
	cvt.u32.u64	%r26, %rd3;
	cvt.u32.u64	%r27, %rd1;
	div.u32 	%r28, %r27, %r26;
	cvt.u64.u32	%rd26, %r28;

BB5_3:
	setp.eq.s64	%p3, %rd2, %rd26;
	@%p3 bra 	BB5_6;

	cvta.to.global.u64 	%rd24, %rd7;
	atom.global.add.u32 	%r29, [%rd24], 1;
	setp.ne.s32	%p4, %r29, 0;
	@%p4 bra 	BB5_6;

	st.global.u64 	[%rd24+8], %rd1;
	st.global.u64 	[%rd24+16], %rd3;
	st.global.u64 	[%rd24+24], %rd2;
	st.global.u64 	[%rd24+32], %rd26;

BB5_6:
	ret;
}

	// .globl	_Z19FastDivHeavyOldTestPKxiPx
.visible .entry _Z19FastDivHeavyOldTestPKxiPx(
	.param .u64 _Z19FastDivHeavyOldTestPKxiPx_param_0,
	.param .u32 _Z19FastDivHeavyOldTestPKxiPx_param_1,
	.param .u64 _Z19FastDivHeavyOldTestPKxiPx_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<19>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd8, [_Z19FastDivHeavyOldTestPKxiPx_param_0];
	ld.param.u32 	%r7, [_Z19FastDivHeavyOldTestPKxiPx_param_1];
	ld.param.u64 	%rd7, [_Z19FastDivHeavyOldTestPKxiPx_param_2];
	mov.u32 	%r8, %tid.x;
	cvta.to.global.u64 	%rd9, %rd8;
	mul.wide.u32 	%rd10, %r8, 8;
	add.s64 	%rd11, %rd9, %rd10;
	mov.u32 	%r9, %ctaid.x;
	add.s32 	%r10, %r9, %r7;
	mov.u32 	%r11, 2;
	max.s32 	%r12, %r11, %r10;
	ld.global.u64 	%rd1, [%rd11];
	mov.b64	{%r13, %r14}, %rd1;
	abs.s64 	%rd12, %rd1;
	abs.s32 	%r15, %r12;
	// inline asm
	// get_reciprocal64 BEGIN
	// inline asm
	clz.b32 	%r16, %r15;
	shl.b32 	%r17, %r15, %r16;
	shr.u32 	%r18, %r17, 8;
	add.s32 	%r1, %r18, 1317011457;
	// inline asm
	mov.b32 	%f1, %r1;
	// inline asm
	and.b32  	%r19, %r17, 255;
	add.s32 	%r20, %r19, -256;
	cvt.rn.f32.s32	%f9, %r20;
	// inline asm
	rcp.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r2, %f2;
	// inline asm
	shl.b32 	%r21, %r16, 23;
	add.s32 	%r22, %r21, %r2;
	add.s32 	%r3, %r22, 553648128;
	// inline asm
	mov.b32 	%f5, %r3;
	// inline asm
	mov.f32 	%f10, 0fBF800000;
	fma.rn.f32 	%f11, %f1, %f2, %f10;
	fma.rn.f32 	%f7, %f9, %f2, %f11;
	and.b32  	%r4, %r3, -4096;
	// inline asm
	mov.b32 	%f6, %r4;
	// inline asm
	// inline asm
	mov.b32 	%r5, %f7;
	// inline asm
	and.b32  	%r6, %r5, -4096;
	// inline asm
	mov.b32 	%f8, %r6;
	// inline asm
	sub.f32 	%f12, %f5, %f6;
	sub.f32 	%f13, %f7, %f8;
	mul.f32 	%f14, %f6, %f8;
	mul.f32 	%f15, %f12, %f8;
	fma.rn.f32 	%f16, %f13, %f5, %f15;
	cvt.rni.s64.f32	%rd13, %f14;
	cvt.rpi.s32.f32	%r23, %f16;
	add.f32 	%f17, %f14, %f16;
	mul.f32 	%f18, %f7, %f17;
	cvt.rmi.s32.f32	%r24, %f18;
	sub.s32 	%r25, %r23, %r24;
	and.b32  	%r26, %r2, 16777215;
	cvt.u64.u32	%rd14, %r26;
	add.s32 	%r27, %r16, 9;
	shl.b64 	%rd15, %rd14, %r27;
	cvt.s64.s32	%rd16, %r25;
	add.s64 	%rd17, %rd16, %rd13;
	shr.s64 	%rd18, %rd17, 2;
	sub.s64 	%rd19, %rd15, %rd18;
	// inline asm
	// get_reciprocal64 END
	// inline asm
	mul.hi.u64 	%rd20, %rd12, %rd19;
	cvt.u64.u32	%rd21, %r15;
	mul.lo.s64 	%rd22, %rd20, %rd21;
	sub.s64 	%rd23, %rd12, %rd22;
	shr.u64 	%rd24, %rd23, 63;
	setp.ge.s64	%p1, %rd23, %rd21;
	selp.u64	%rd25, 1, 0, %p1;
	sub.s64 	%rd26, %rd25, %rd24;
	add.s64 	%rd27, %rd26, %rd20;
	xor.b32  	%r28, %r12, %r14;
	setp.lt.s32	%p2, %r28, 0;
	neg.s64 	%rd28, %rd27;
	selp.b64	%rd2, %rd28, %rd27, %p2;
	cvt.s64.s32	%rd3, %r12;
	or.b64  	%rd29, %rd1, %rd3;
	and.b64  	%rd30, %rd29, -4294967296;
	setp.eq.s64	%p3, %rd30, 0;
	@%p3 bra 	BB6_2;

	div.s64 	%rd33, %rd1, %rd3;
	bra.uni 	BB6_3;

BB6_2:
	cvt.u32.u64	%r29, %rd3;
	cvt.u32.u64	%r30, %rd1;
	div.u32 	%r31, %r30, %r29;
	cvt.u64.u32	%rd33, %r31;

BB6_3:
	setp.eq.s64	%p4, %rd2, %rd33;
	@%p4 bra 	BB6_6;

	cvta.to.global.u64 	%rd31, %rd7;
	atom.global.add.u32 	%r32, [%rd31], 1;
	setp.ne.s32	%p5, %r32, 0;
	@%p5 bra 	BB6_6;

	st.global.u64 	[%rd31+8], %rd1;
	st.global.u64 	[%rd31+16], %rd3;
	st.global.u64 	[%rd31+24], %rd2;
	st.global.u64 	[%rd31+32], %rd33;

BB6_6:
	ret;
}

	// .globl	_Z12FastDivBenchPKyjPy
.visible .entry _Z12FastDivBenchPKyjPy(
	.param .u64 _Z12FastDivBenchPKyjPy_param_0,
	.param .u32 _Z12FastDivBenchPKyjPy_param_1,
	.param .u64 _Z12FastDivBenchPKyjPy_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd2, [_Z12FastDivBenchPKyjPy_param_0];
	ld.param.u32 	%r4, [_Z12FastDivBenchPKyjPy_param_1];
	ld.param.u64 	%rd1, [_Z12FastDivBenchPKyjPy_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	mov.u32 	%r5, %tid.x;
	mul.wide.u32 	%rd4, %r5, 8;
	add.s64 	%rd5, %rd3, %rd4;
	mov.u32 	%r6, %ctaid.x;
	add.s32 	%r7, %r6, %r4;
	setp.eq.s32	%p1, %r7, -2147483648;
	selp.b32	%r8, -2147483647, %r7, %p1;
	ld.global.u64 	%rd6, [%rd5];
	mov.b64	{%r9, %r10}, %rd6;
	shr.u32 	%r11, %r8, 8;
	add.s32 	%r1, %r11, 1317011456;
	// inline asm
	mov.b32 	%f1, %r1;
	// inline asm
	and.b32  	%r12, %r8, 255;
	cvt.rn.f32.u32	%f6, %r12;
	// inline asm
	rcp.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r2, %f2;
	// inline asm
	add.s32 	%r3, %r2, 536870912;
	// inline asm
	mov.b32 	%f5, %r3;
	// inline asm
	mov.f32 	%f7, 0fBF800000;
	fma.rn.f32 	%f8, %f1, %f2, %f7;
	fma.rn.f32 	%f9, %f6, %f2, %f8;
	shl.b32 	%r13, %r2, 9;
	mul.f32 	%f10, %f5, %f9;
	cvt.rni.s32.f32	%r14, %f10;
	sub.s32 	%r15, %r13, %r14;
	mul.hi.u32 	%r16, %r9, %r15;
	cvt.u64.u32	%rd7, %r16;
	mul.wide.u32 	%rd8, %r15, %r10;
	add.s64 	%rd9, %rd8, %rd7;
	add.s64 	%rd10, %rd9, %rd6;
	setp.lt.u64	%p2, %rd10, %rd9;
	mov.b64	{%r17, %r18}, %rd10;
	cvt.u64.u32	%rd11, %r8;
	mul.wide.u32 	%rd12, %r18, %r8;
	sub.s64 	%rd13, %rd6, %rd12;
	mov.b64	{%r19, %r20}, %rd13;
	selp.b32	%r21, %r8, 0, %p2;
	sub.s32 	%r22, %r20, %r21;
	mov.b64	%rd14, {%r19, %r22};
	setp.ge.s64	%p3, %rd14, %rd11;
	selp.u32	%r23, 1, 0, %p3;
	shr.s32 	%r24, %r22, 31;
	add.s32 	%r25, %r24, %r18;
	setp.gt.s32	%p4, %r22, -1;
	selp.b32	%r26, 0, %r8, %p4;
	add.s32 	%r27, %r26, %r19;
	neg.s32 	%r28, %r8;
	selp.b32	%r29, %r28, 0, %p3;
	add.s32 	%r30, %r27, %r29;
	add.s32 	%r31, %r25, %r23;
	mov.b64	%rd15, {%r31, %r30};
	setp.ne.s64	%p5, %rd15, 1;
	@%p5 bra 	BB7_2;

	cvta.to.global.u64 	%rd16, %rd1;
	mov.u64 	%rd17, 0;
	st.global.u64 	[%rd16], %rd17;

BB7_2:
	ret;
}

	// .globl	_Z17FastDivHeavyBenchPKxiPx
.visible .entry _Z17FastDivHeavyBenchPKxiPx(
	.param .u64 _Z17FastDivHeavyBenchPKxiPx_param_0,
	.param .u32 _Z17FastDivHeavyBenchPKxiPx_param_1,
	.param .u64 _Z17FastDivHeavyBenchPKxiPx_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd2, [_Z17FastDivHeavyBenchPKxiPx_param_0];
	ld.param.u32 	%r3, [_Z17FastDivHeavyBenchPKxiPx_param_1];
	ld.param.u64 	%rd1, [_Z17FastDivHeavyBenchPKxiPx_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	mov.u32 	%r4, %tid.x;
	mul.wide.u32 	%rd4, %r4, 8;
	add.s64 	%rd5, %rd3, %rd4;
	mov.u32 	%r5, %ctaid.x;
	add.s32 	%r6, %r5, %r3;
	mov.u32 	%r7, 2;
	max.s32 	%r8, %r7, %r6;
	ld.global.u64 	%rd6, [%rd5];
	mov.b64	{%r9, %r10}, %rd6;
	abs.s64 	%rd7, %rd6;
	mov.b64	{%r11, %r12}, %rd7;
	abs.s32 	%r13, %r8;
	cvt.rn.f32.s32	%f2, %r13;
	// inline asm
	rcp.approx.f32 %f1, %f2;
	// inline asm
	// inline asm
	mov.b32 	%r1, %f1;
	// inline asm
	add.s32 	%r2, %r1, 268435456;
	// inline asm
	mov.b32 	%f4, %r2;
	// inline asm
	cvt.rn.f32.s32	%f5, %r12;
	mul.f32 	%f6, %f4, %f5;
	cvt.rni.u64.f32	%rd8, %f6;
	cvt.u64.u32	%rd9, %r13;
	mul.lo.s64 	%rd10, %rd8, %rd9;
	sub.s64 	%rd11, %rd7, %rd10;
	mov.b64	{%r14, %r15}, %rd11;
	cvt.rm.f32.s64	%f7, %rd11;
	mul.f32 	%f8, %f1, %f7;
	cvt.rni.s64.f32	%rd12, %f8;
	mov.b64	{%r16, %r17}, %rd12;
	mul.lo.s32 	%r18, %r16, %r13;
	sub.s32 	%r19, %r14, %r18;
	cvt.rn.f32.s32	%f9, %r19;
	mul.f32 	%f10, %f1, %f9;
	cvt.rni.s32.f32	%r20, %f10;
	mul.lo.s32 	%r21, %r20, %r13;
	sub.s32 	%r22, %r19, %r21;
	shr.s32 	%r23, %r22, 31;
	add.s32 	%r24, %r23, %r20;
	add.s64 	%rd13, %rd12, %rd8;
	cvt.s64.s32	%rd14, %r24;
	add.s64 	%rd15, %rd13, %rd14;
	xor.b32  	%r25, %r8, %r10;
	setp.lt.s32	%p1, %r25, 0;
	neg.s64 	%rd16, %rd15;
	selp.b64	%rd17, %rd16, %rd15, %p1;
	setp.ne.s64	%p2, %rd17, 1;
	@%p2 bra 	BB8_2;

	cvta.to.global.u64 	%rd18, %rd1;
	mov.u64 	%rd19, 0;
	st.global.u64 	[%rd18], %rd19;

BB8_2:
	ret;
}

	// .globl	_Z20FastDivHeavyOldBenchPKxiPx
.visible .entry _Z20FastDivHeavyOldBenchPKxiPx(
	.param .u64 _Z20FastDivHeavyOldBenchPKxiPx_param_0,
	.param .u32 _Z20FastDivHeavyOldBenchPKxiPx_param_1,
	.param .u64 _Z20FastDivHeavyOldBenchPKxiPx_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<19>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<27>;


	ld.param.u64 	%rd2, [_Z20FastDivHeavyOldBenchPKxiPx_param_0];
	ld.param.u32 	%r7, [_Z20FastDivHeavyOldBenchPKxiPx_param_1];
	ld.param.u64 	%rd1, [_Z20FastDivHeavyOldBenchPKxiPx_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	mov.u32 	%r8, %tid.x;
	mul.wide.u32 	%rd4, %r8, 8;
	add.s64 	%rd5, %rd3, %rd4;
	mov.u32 	%r9, %ctaid.x;
	add.s32 	%r10, %r9, %r7;
	mov.u32 	%r11, 2;
	max.s32 	%r12, %r11, %r10;
	ld.global.u64 	%rd6, [%rd5];
	mov.b64	{%r13, %r14}, %rd6;
	abs.s64 	%rd7, %rd6;
	abs.s32 	%r15, %r12;
	// inline asm
	// get_reciprocal64 BEGIN
	// inline asm
	clz.b32 	%r16, %r15;
	shl.b32 	%r17, %r15, %r16;
	shr.u32 	%r18, %r17, 8;
	add.s32 	%r1, %r18, 1317011457;
	// inline asm
	mov.b32 	%f1, %r1;
	// inline asm
	and.b32  	%r19, %r17, 255;
	add.s32 	%r20, %r19, -256;
	cvt.rn.f32.s32	%f9, %r20;
	// inline asm
	rcp.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r2, %f2;
	// inline asm
	shl.b32 	%r21, %r16, 23;
	add.s32 	%r22, %r21, %r2;
	add.s32 	%r3, %r22, 553648128;
	// inline asm
	mov.b32 	%f5, %r3;
	// inline asm
	mov.f32 	%f10, 0fBF800000;
	fma.rn.f32 	%f11, %f1, %f2, %f10;
	fma.rn.f32 	%f7, %f9, %f2, %f11;
	and.b32  	%r4, %r3, -4096;
	// inline asm
	mov.b32 	%f6, %r4;
	// inline asm
	// inline asm
	mov.b32 	%r5, %f7;
	// inline asm
	and.b32  	%r6, %r5, -4096;
	// inline asm
	mov.b32 	%f8, %r6;
	// inline asm
	sub.f32 	%f12, %f5, %f6;
	sub.f32 	%f13, %f7, %f8;
	mul.f32 	%f14, %f6, %f8;
	mul.f32 	%f15, %f12, %f8;
	fma.rn.f32 	%f16, %f13, %f5, %f15;
	cvt.rni.s64.f32	%rd8, %f14;
	cvt.rpi.s32.f32	%r23, %f16;
	add.f32 	%f17, %f14, %f16;
	mul.f32 	%f18, %f7, %f17;
	cvt.rmi.s32.f32	%r24, %f18;
	sub.s32 	%r25, %r23, %r24;
	and.b32  	%r26, %r2, 16777215;
	cvt.u64.u32	%rd9, %r26;
	add.s32 	%r27, %r16, 9;
	shl.b64 	%rd10, %rd9, %r27;
	cvt.s64.s32	%rd11, %r25;
	add.s64 	%rd12, %rd11, %rd8;
	shr.s64 	%rd13, %rd12, 2;
	sub.s64 	%rd14, %rd10, %rd13;
	// inline asm
	// get_reciprocal64 END
	// inline asm
	mul.hi.u64 	%rd15, %rd7, %rd14;
	cvt.u64.u32	%rd16, %r15;
	mul.lo.s64 	%rd17, %rd15, %rd16;
	sub.s64 	%rd18, %rd7, %rd17;
	shr.u64 	%rd19, %rd18, 63;
	setp.ge.s64	%p1, %rd18, %rd16;
	selp.u64	%rd20, 1, 0, %p1;
	sub.s64 	%rd21, %rd20, %rd19;
	add.s64 	%rd22, %rd21, %rd15;
	xor.b32  	%r28, %r12, %r14;
	setp.lt.s32	%p2, %r28, 0;
	neg.s64 	%rd23, %rd22;
	selp.b64	%rd24, %rd23, %rd22, %p2;
	setp.ne.s64	%p3, %rd24, 1;
	@%p3 bra 	BB9_2;

	cvta.to.global.u64 	%rd25, %rd1;
	mov.u64 	%rd26, 0;
	st.global.u64 	[%rd25], %rd26;

BB9_2:
	ret;
}

	// .globl	_Z18NaiveDivHeavyBenchPKxiPx
.visible .entry _Z18NaiveDivHeavyBenchPKxiPx(
	.param .u64 _Z18NaiveDivHeavyBenchPKxiPx_param_0,
	.param .u32 _Z18NaiveDivHeavyBenchPKxiPx_param_1,
	.param .u64 _Z18NaiveDivHeavyBenchPKxiPx_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd7, [_Z18NaiveDivHeavyBenchPKxiPx_param_0];
	ld.param.u32 	%r1, [_Z18NaiveDivHeavyBenchPKxiPx_param_1];
	ld.param.u64 	%rd6, [_Z18NaiveDivHeavyBenchPKxiPx_param_2];
	cvta.to.global.u64 	%rd8, %rd7;
	mov.u32 	%r2, %tid.x;
	mul.wide.u32 	%rd9, %r2, 8;
	add.s64 	%rd10, %rd8, %rd9;
	mov.u32 	%r3, %ctaid.x;
	add.s32 	%r4, %r3, %r1;
	setp.lt.s32	%p1, %r4, 2;
	cvt.s64.s32	%rd11, %r4;
	selp.b64	%rd1, 2, %rd11, %p1;
	ld.global.u64 	%rd2, [%rd10];
	or.b64  	%rd12, %rd2, %rd1;
	and.b64  	%rd13, %rd12, -4294967296;
	setp.eq.s64	%p2, %rd13, 0;
	@%p2 bra 	BB10_2;

	div.s64 	%rd16, %rd2, %rd1;
	bra.uni 	BB10_3;

BB10_2:
	cvt.u32.u64	%r5, %rd1;
	cvt.u32.u64	%r6, %rd2;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32	%rd16, %r7;

BB10_3:
	setp.ne.s64	%p3, %rd16, 1;
	@%p3 bra 	BB10_5;

	cvta.to.global.u64 	%rd14, %rd6;
	mov.u64 	%rd15, 0;
	st.global.u64 	[%rd14], %rd15;

BB10_5:
	ret;
}

	// .globl	_Z12FastSqrtTestjPy
.visible .entry _Z12FastSqrtTestjPy(
	.param .u32 _Z12FastSqrtTestjPy_param_0,
	.param .u64 _Z12FastSqrtTestjPy_param_1
)
{
	.reg .pred 	%p<18>;
	.reg .f32 	%f<51>;
	.reg .b32 	%r<109>;
	.reg .b64 	%rd<99>;


	ld.param.u32 	%r3, [_Z12FastSqrtTestjPy_param_0];
	ld.param.u64 	%rd15, [_Z12FastSqrtTestjPy_param_1];
	cvta.to.global.u64 	%rd1, %rd15;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r4, %r5, %r3;
	mov.u32 	%r7, %tid.x;
	add.s32 	%r1, %r6, %r7;
	setp.gt.s32	%p1, %r1, 1779033702;
	@%p1 bra 	BB11_13;
	bra.uni 	BB11_1;

BB11_13:
	setp.ne.s32	%p14, %r1, 1779033703;
	@%p14 bra 	BB11_17;

	mov.u32 	%r92, 1610612735;
	// inline asm
	mov.b32 	%f41, %r92;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f42, %f41;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f44, %f41;
	// inline asm
	// inline asm
	mov.b32 	%r93, %f42;
	// inline asm
	add.s32 	%r94, %r93, 268435456;
	// inline asm
	mov.b32 	%f47, %r94;
	// inline asm
	// inline asm
	mov.b32 	%r95, %f44;
	// inline asm
	add.s32 	%r96, %r95, -1325400064;
	mul.wide.u32 	%rd80, %r96, %r96;
	shl.b64 	%rd81, %rd80, 18;
	not.b64 	%rd82, %rd81;
	mov.b64	{%r97, %r98}, %rd82;
	cvt.rn.f32.s32	%f49, %r98;
	mul.f32 	%f50, %f47, %f49;
	shl.b32 	%r99, %r96, 10;
	cvt.rni.s32.f32	%r100, %f50;
	add.s32 	%r101, %r99, %r100;
	shr.u32 	%r102, %r101, 1;
	and.b32  	%r103, %r101, 1;
	cvt.u64.u32	%rd83, %r102;
	add.s32 	%r104, %r102, %r103;
	mul.wide.u32 	%rd84, %r104, %r102;
	cvt.u64.u32	%rd85, %r101;
	shl.b64 	%rd86, %rd85, 32;
	add.s64 	%rd87, %rd86, %rd84;
	add.s64 	%rd88, %rd87, 1;
	cvt.u64.u32	%rd89, %r103;
	add.s64 	%rd90, %rd88, %rd89;
	setp.gt.s64	%p15, %rd90, 0;
	selp.b32	%r105, -1, 0, %p15;
	add.s32 	%r106, %r105, %r101;
	add.s64 	%rd91, %rd83, %rd88;
	add.s64 	%rd92, %rd91, 4294967296;
	shr.u64 	%rd93, %rd92, 63;
	cvt.u32.u64	%r107, %rd93;
	add.s32 	%r2, %r106, %r107;
	setp.eq.s32	%p16, %r2, -736899889;
	@%p16 bra 	BB11_17;

	atom.global.add.u32 	%r108, [%rd1], 1;
	setp.ne.s32	%p17, %r108, 0;
	@%p17 bra 	BB11_17;

	cvt.u64.u32	%rd95, %r2;
	mov.u64 	%rd96, 1779033703;
	st.global.u64 	[%rd1+8], %rd96;
	mov.u64 	%rd97, -1;
	st.global.u64 	[%rd1+16], %rd97;
	mov.u64 	%rd98, 3558067407;
	st.global.u64 	[%rd1+24], %rd98;
	st.global.u64 	[%rd1+32], %rd95;
	bra.uni 	BB11_17;

BB11_1:
	cvt.s64.s32	%rd2, %r1;
	add.s64 	%rd3, %rd2, 4294967296;
	mul.lo.s64 	%rd4, %rd3, %rd3;
	mov.b64	{%r12, %r13}, %rd4;
	shr.u32 	%r14, %r13, 9;
	add.s32 	%r8, %r14, 1602224128;
	// inline asm
	mov.b32 	%f1, %r8;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f4, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r9, %f2;
	// inline asm
	add.s32 	%r10, %r9, 268435456;
	// inline asm
	mov.b32 	%f7, %r10;
	// inline asm
	// inline asm
	mov.b32 	%r11, %f4;
	// inline asm
	add.s32 	%r15, %r11, -1325400064;
	mul.wide.u32 	%rd16, %r15, %r15;
	shl.b64 	%rd17, %rd16, 18;
	sub.s64 	%rd18, %rd4, %rd17;
	mov.b64	{%r16, %r17}, %rd18;
	cvt.rn.f32.s32	%f9, %r17;
	mul.f32 	%f10, %f7, %f9;
	shl.b32 	%r18, %r15, 10;
	cvt.rni.s32.f32	%r19, %f10;
	add.s32 	%r20, %r18, %r19;
	shr.u32 	%r21, %r20, 1;
	and.b32  	%r22, %r20, 1;
	cvt.u64.u32	%rd19, %r21;
	add.s32 	%r23, %r21, %r22;
	mul.wide.u32 	%rd20, %r23, %r21;
	cvt.u64.u32	%rd21, %r20;
	shl.b64 	%rd22, %rd21, 32;
	sub.s64 	%rd23, %rd22, %rd4;
	add.s64 	%rd24, %rd20, %rd23;
	cvt.u64.u32	%rd25, %r22;
	add.s64 	%rd26, %rd24, %rd25;
	setp.gt.s64	%p2, %rd26, 0;
	selp.b32	%r24, -1, 0, %p2;
	add.s32 	%r25, %r24, %r20;
	add.s64 	%rd27, %rd19, %rd24;
	add.s64 	%rd28, %rd27, 4294967296;
	shr.u64 	%rd29, %rd28, 63;
	cvt.u32.u64	%r26, %rd29;
	add.s32 	%r27, %r25, %r26;
	cvt.u64.u32	%rd5, %r27;
	shl.b64 	%rd6, %rd3, 1;
	add.s64 	%rd7, %rd6, -8589934592;
	setp.eq.s64	%p3, %rd5, %rd7;
	@%p3 bra 	BB11_4;

	atom.global.add.u32 	%r28, [%rd1], 1;
	setp.ne.s32	%p4, %r28, 0;
	@%p4 bra 	BB11_4;

	st.global.u64 	[%rd1+8], %rd2;
	st.global.u64 	[%rd1+16], %rd4;
	st.global.u64 	[%rd1+24], %rd7;
	st.global.u64 	[%rd1+32], %rd5;

BB11_4:
	add.s64 	%rd8, %rd4, %rd3;
	mov.b64	{%r33, %r34}, %rd8;
	shr.u32 	%r35, %r34, 9;
	add.s32 	%r29, %r35, 1602224128;
	// inline asm
	mov.b32 	%f11, %r29;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f12, %f11;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f14, %f11;
	// inline asm
	// inline asm
	mov.b32 	%r30, %f12;
	// inline asm
	add.s32 	%r31, %r30, 268435456;
	// inline asm
	mov.b32 	%f17, %r31;
	// inline asm
	// inline asm
	mov.b32 	%r32, %f14;
	// inline asm
	add.s32 	%r36, %r32, -1325400064;
	mul.wide.u32 	%rd31, %r36, %r36;
	shl.b64 	%rd32, %rd31, 18;
	sub.s64 	%rd33, %rd8, %rd32;
	mov.b64	{%r37, %r38}, %rd33;
	cvt.rn.f32.s32	%f19, %r38;
	mul.f32 	%f20, %f17, %f19;
	shl.b32 	%r39, %r36, 10;
	cvt.rni.s32.f32	%r40, %f20;
	add.s32 	%r41, %r39, %r40;
	shr.u32 	%r42, %r41, 1;
	and.b32  	%r43, %r41, 1;
	cvt.u64.u32	%rd34, %r42;
	add.s32 	%r44, %r42, %r43;
	mul.wide.u32 	%rd35, %r44, %r42;
	cvt.u64.u32	%rd36, %r41;
	shl.b64 	%rd37, %rd36, 32;
	sub.s64 	%rd38, %rd37, %rd8;
	add.s64 	%rd39, %rd35, %rd38;
	cvt.u64.u32	%rd40, %r43;
	add.s64 	%rd41, %rd39, %rd40;
	setp.gt.s64	%p5, %rd41, 0;
	selp.b32	%r45, -1, 0, %p5;
	add.s32 	%r46, %r45, %r41;
	add.s64 	%rd42, %rd34, %rd39;
	add.s64 	%rd43, %rd42, 4294967296;
	shr.u64 	%rd44, %rd43, 63;
	cvt.u32.u64	%r47, %rd44;
	add.s32 	%r48, %r46, %r47;
	cvt.u64.u32	%rd9, %r48;
	setp.eq.s64	%p6, %rd9, %rd7;
	@%p6 bra 	BB11_7;

	atom.global.add.u32 	%r49, [%rd1], 1;
	setp.ne.s32	%p7, %r49, 0;
	@%p7 bra 	BB11_7;

	st.global.u64 	[%rd1+8], %rd2;
	st.global.u64 	[%rd1+16], %rd8;
	st.global.u64 	[%rd1+24], %rd7;
	st.global.u64 	[%rd1+32], %rd9;

BB11_7:
	add.s64 	%rd10, %rd8, 1;
	mov.b64	{%r54, %r55}, %rd10;
	shr.u32 	%r56, %r55, 9;
	add.s32 	%r50, %r56, 1602224128;
	// inline asm
	mov.b32 	%f21, %r50;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f22, %f21;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f24, %f21;
	// inline asm
	// inline asm
	mov.b32 	%r51, %f22;
	// inline asm
	add.s32 	%r52, %r51, 268435456;
	// inline asm
	mov.b32 	%f27, %r52;
	// inline asm
	// inline asm
	mov.b32 	%r53, %f24;
	// inline asm
	add.s32 	%r57, %r53, -1325400064;
	mul.wide.u32 	%rd46, %r57, %r57;
	shl.b64 	%rd47, %rd46, 18;
	sub.s64 	%rd48, %rd10, %rd47;
	mov.b64	{%r58, %r59}, %rd48;
	cvt.rn.f32.s32	%f29, %r59;
	mul.f32 	%f30, %f27, %f29;
	shl.b32 	%r60, %r57, 10;
	cvt.rni.s32.f32	%r61, %f30;
	add.s32 	%r62, %r60, %r61;
	shr.u32 	%r63, %r62, 1;
	and.b32  	%r64, %r62, 1;
	cvt.u64.u32	%rd49, %r63;
	add.s32 	%r65, %r63, %r64;
	mul.wide.u32 	%rd50, %r65, %r63;
	cvt.u64.u32	%rd51, %r62;
	shl.b64 	%rd52, %rd51, 32;
	sub.s64 	%rd53, %rd52, %rd10;
	add.s64 	%rd54, %rd50, %rd53;
	cvt.u64.u32	%rd55, %r64;
	add.s64 	%rd56, %rd54, %rd55;
	setp.gt.s64	%p8, %rd56, 0;
	selp.b32	%r66, -1, 0, %p8;
	add.s32 	%r67, %r66, %r62;
	add.s64 	%rd57, %rd49, %rd54;
	add.s64 	%rd58, %rd57, 4294967296;
	shr.u64 	%rd59, %rd58, 63;
	cvt.u32.u64	%r68, %rd59;
	add.s32 	%r69, %r67, %r68;
	cvt.u64.u32	%rd60, %r69;
	add.s64 	%rd11, %rd6, -8589934591;
	setp.eq.s64	%p9, %rd60, %rd11;
	@%p9 bra 	BB11_10;

	atom.global.add.u32 	%r70, [%rd1], 1;
	setp.ne.s32	%p10, %r70, 0;
	@%p10 bra 	BB11_10;

	st.global.u64 	[%rd1+8], %rd2;
	st.global.u64 	[%rd1+16], %rd10;
	st.global.u64 	[%rd1+24], %rd11;
	st.global.u64 	[%rd1+32], %rd9;

BB11_10:
	add.s64 	%rd62, %rd2, 4294967297;
	mul.lo.s64 	%rd63, %rd62, %rd62;
	add.s64 	%rd12, %rd63, -1;
	mov.b64	{%r75, %r76}, %rd12;
	shr.u32 	%r77, %r76, 9;
	add.s32 	%r71, %r77, 1602224128;
	// inline asm
	mov.b32 	%f31, %r71;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f32, %f31;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f34, %f31;
	// inline asm
	// inline asm
	mov.b32 	%r72, %f32;
	// inline asm
	add.s32 	%r73, %r72, 268435456;
	// inline asm
	mov.b32 	%f37, %r73;
	// inline asm
	// inline asm
	mov.b32 	%r74, %f34;
	// inline asm
	add.s32 	%r78, %r74, -1325400064;
	mul.wide.u32 	%rd64, %r78, %r78;
	shl.b64 	%rd65, %rd64, 18;
	sub.s64 	%rd66, %rd12, %rd65;
	mov.b64	{%r79, %r80}, %rd66;
	cvt.rn.f32.s32	%f39, %r80;
	mul.f32 	%f40, %f37, %f39;
	shl.b32 	%r81, %r78, 10;
	cvt.rni.s32.f32	%r82, %f40;
	add.s32 	%r83, %r81, %r82;
	shr.u32 	%r84, %r83, 1;
	and.b32  	%r85, %r83, 1;
	cvt.u64.u32	%rd67, %r84;
	add.s32 	%r86, %r84, %r85;
	mul.wide.u32 	%rd68, %r86, %r84;
	cvt.u64.u32	%rd69, %r83;
	shl.b64 	%rd70, %rd69, 32;
	sub.s64 	%rd71, %rd70, %rd12;
	add.s64 	%rd72, %rd68, %rd71;
	cvt.u64.u32	%rd73, %r85;
	add.s64 	%rd74, %rd72, %rd73;
	setp.gt.s64	%p11, %rd74, 0;
	selp.b32	%r87, -1, 0, %p11;
	add.s32 	%r88, %r87, %r83;
	add.s64 	%rd75, %rd67, %rd72;
	add.s64 	%rd76, %rd75, 4294967296;
	shr.u64 	%rd77, %rd76, 63;
	cvt.u32.u64	%r89, %rd77;
	add.s32 	%r90, %r88, %r89;
	cvt.u64.u32	%rd13, %r90;
	shl.b64 	%rd78, %rd62, 1;
	add.s64 	%rd14, %rd78, -8589934593;
	setp.eq.s64	%p12, %rd13, %rd14;
	@%p12 bra 	BB11_17;

	atom.global.add.u32 	%r91, [%rd1], 1;
	setp.ne.s32	%p13, %r91, 0;
	@%p13 bra 	BB11_17;

	st.global.u64 	[%rd1+8], %rd2;
	st.global.u64 	[%rd1+16], %rd12;
	st.global.u64 	[%rd1+24], %rd14;
	st.global.u64 	[%rd1+32], %rd13;

BB11_17:
	ret;
}


