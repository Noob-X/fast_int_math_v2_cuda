//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-24330188
// Cuda compilation tools, release 9.2, V9.2.148
// Based on LLVM 3.4svn
//

.version 6.2
.target sm_30
.address_size 64

	// .globl	_Z15DummyFastDivPTXPKyPKjPy

.visible .entry _Z15DummyFastDivPTXPKyPKjPy(
	.param .u64 _Z15DummyFastDivPTXPKyPKjPy_param_0,
	.param .u64 _Z15DummyFastDivPTXPKyPKjPy_param_1,
	.param .u64 _Z15DummyFastDivPTXPKyPKjPy_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd1, [_Z15DummyFastDivPTXPKyPKjPy_param_0];
	ld.param.u64 	%rd2, [_Z15DummyFastDivPTXPKyPKjPy_param_1];
	ld.param.u64 	%rd3, [_Z15DummyFastDivPTXPKyPKjPy_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r7, %r5, %r4, %r6;
	mul.wide.s32 	%rd7, %r7, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd9, [%rd8];
	mul.wide.s32 	%rd10, %r7, 4;
	add.s64 	%rd11, %rd5, %rd10;
	ld.global.u32 	%r8, [%rd11];
	// inline asm
	// fast_div_v2 BEGIN
	// inline asm
	mov.b64	{%r9, %r10}, %rd9;
	shr.u32 	%r11, %r8, 8;
	add.s32 	%r1, %r11, 1317011456;
	// inline asm
	mov.b32 	%f1, %r1;
	// inline asm
	and.b32  	%r12, %r8, 255;
	cvt.rn.f32.u32	%f6, %r12;
	// inline asm
	rcp.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r2, %f2;
	// inline asm
	add.s32 	%r3, %r2, 536870912;
	// inline asm
	mov.b32 	%f5, %r3;
	// inline asm
	mov.f32 	%f7, 0fBF800000;
	fma.rn.f32 	%f8, %f1, %f2, %f7;
	fma.rn.f32 	%f9, %f6, %f2, %f8;
	shl.b32 	%r13, %r2, 9;
	mul.f32 	%f10, %f5, %f9;
	cvt.rni.s32.f32	%r14, %f10;
	sub.s32 	%r15, %r13, %r14;
	mul.hi.u32 	%r16, %r9, %r15;
	cvt.u64.u32	%rd12, %r16;
	mul.wide.u32 	%rd13, %r15, %r10;
	add.s64 	%rd14, %rd13, %rd12;
	add.s64 	%rd15, %rd14, %rd9;
	setp.lt.u64	%p1, %rd15, %rd14;
	mov.b64	{%r17, %r18}, %rd15;
	cvt.u64.u32	%rd16, %r8;
	mul.wide.u32 	%rd17, %r18, %r8;
	sub.s64 	%rd18, %rd9, %rd17;
	mov.b64	{%r19, %r20}, %rd18;
	selp.b32	%r21, %r8, 0, %p1;
	sub.s32 	%r22, %r20, %r21;
	mov.b64	%rd19, {%r19, %r22};
	setp.ge.s64	%p2, %rd19, %rd16;
	selp.u32	%r23, 1, 0, %p2;
	shr.s32 	%r24, %r22, 31;
	add.s32 	%r25, %r24, %r18;
	setp.gt.s32	%p3, %r22, -1;
	selp.b32	%r26, 0, %r8, %p3;
	add.s32 	%r27, %r26, %r19;
	neg.s32 	%r28, %r8;
	selp.b32	%r29, %r28, 0, %p2;
	add.s32 	%r30, %r27, %r29;
	add.s32 	%r31, %r25, %r23;
	// inline asm
	// fast_div_v2 END
	// inline asm
	add.s64 	%rd20, %rd4, %rd7;
	st.global.v2.u32 	[%rd20], {%r31, %r30};
	ret;
}

	// .globl	_Z16DummyFastSqrtPTXPKyPj
.visible .entry _Z16DummyFastSqrtPTXPKyPj(
	.param .u64 _Z16DummyFastSqrtPTXPKyPj_param_0,
	.param .u64 _Z16DummyFastSqrtPTXPKyPj_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd1, [_Z16DummyFastSqrtPTXPKyPj_param_0];
	ld.param.u64 	%rd2, [_Z16DummyFastSqrtPTXPKyPj_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r8, %r6, %r5, %r7;
	mul.wide.s32 	%rd5, %r8, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u64 	%rd7, [%rd6];
	// inline asm
	// fast_sqrt_v2 BEGIN
	// inline asm
	mov.b64	{%r9, %r10}, %rd7;
	shr.u32 	%r11, %r10, 9;
	add.s32 	%r1, %r11, 1602224128;
	// inline asm
	mov.b32 	%f1, %r1;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f4, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r2, %f2;
	// inline asm
	add.s32 	%r3, %r2, 268435456;
	// inline asm
	mov.b32 	%f7, %r3;
	// inline asm
	// inline asm
	mov.b32 	%r4, %f4;
	// inline asm
	add.s32 	%r12, %r4, -1325400064;
	mul.wide.u32 	%rd8, %r12, %r12;
	shl.b64 	%rd9, %rd8, 18;
	sub.s64 	%rd10, %rd7, %rd9;
	mov.b64	{%r13, %r14}, %rd10;
	cvt.rn.f32.s32	%f9, %r14;
	mul.f32 	%f10, %f7, %f9;
	shl.b32 	%r15, %r12, 10;
	cvt.rni.s32.f32	%r16, %f10;
	add.s32 	%r17, %r15, %r16;
	shr.u32 	%r18, %r17, 1;
	and.b32  	%r19, %r17, 1;
	cvt.u64.u32	%rd11, %r18;
	add.s32 	%r20, %r18, %r19;
	mul.wide.u32 	%rd12, %r20, %r18;
	cvt.u64.u32	%rd13, %r17;
	shl.b64 	%rd14, %rd13, 32;
	sub.s64 	%rd15, %rd14, %rd7;
	add.s64 	%rd16, %rd12, %rd15;
	cvt.u64.u32	%rd17, %r19;
	add.s64 	%rd18, %rd16, %rd17;
	setp.gt.s64	%p1, %rd18, 0;
	selp.b32	%r21, -1, 0, %p1;
	add.s32 	%r22, %r21, %r17;
	add.s64 	%rd19, %rd11, %rd16;
	add.s64 	%rd20, %rd19, 4294967296;
	shr.u64 	%rd21, %rd20, 63;
	cvt.u32.u64	%r23, %rd21;
	add.s32 	%r24, %r22, %r23;
	// inline asm
	// fast_sqrt_v2 END
	// inline asm
	mul.wide.s32 	%rd22, %r8, 4;
	add.s64 	%rd23, %rd3, %rd22;
	st.global.u32 	[%rd23], %r24;
	ret;
}

	// .globl	_Z11FastDivTestPKyjPy
.visible .entry _Z11FastDivTestPKyjPy(
	.param .u64 _Z11FastDivTestPKyjPy_param_0,
	.param .u32 _Z11FastDivTestPKyjPy_param_1,
	.param .u64 _Z11FastDivTestPKyjPy_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<30>;


	ld.param.u64 	%rd12, [_Z11FastDivTestPKyjPy_param_0];
	ld.param.u32 	%r4, [_Z11FastDivTestPKyjPy_param_1];
	ld.param.u64 	%rd11, [_Z11FastDivTestPKyjPy_param_2];
	mov.u32 	%r5, %tid.x;
	cvta.to.global.u64 	%rd13, %rd12;
	mul.wide.u32 	%rd14, %r5, 8;
	add.s64 	%rd15, %rd13, %rd14;
	mov.u32 	%r6, %ctaid.x;
	add.s32 	%r7, %r6, %r4;
	setp.eq.s32	%p1, %r7, -2147483648;
	selp.b32	%r8, -2147483647, %r7, %p1;
	ld.global.u64 	%rd1, [%rd15];
	mov.b64	{%r9, %r10}, %rd1;
	shr.u32 	%r11, %r8, 8;
	add.s32 	%r1, %r11, 1317011456;
	// inline asm
	mov.b32 	%f1, %r1;
	// inline asm
	and.b32  	%r12, %r8, 255;
	cvt.rn.f32.u32	%f6, %r12;
	// inline asm
	rcp.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r2, %f2;
	// inline asm
	add.s32 	%r3, %r2, 536870912;
	// inline asm
	mov.b32 	%f5, %r3;
	// inline asm
	mov.f32 	%f7, 0fBF800000;
	fma.rn.f32 	%f8, %f1, %f2, %f7;
	fma.rn.f32 	%f9, %f6, %f2, %f8;
	shl.b32 	%r13, %r2, 9;
	mul.f32 	%f10, %f5, %f9;
	cvt.rni.s32.f32	%r14, %f10;
	sub.s32 	%r15, %r13, %r14;
	mul.hi.u32 	%r16, %r9, %r15;
	cvt.u64.u32	%rd16, %r16;
	mul.wide.u32 	%rd17, %r15, %r10;
	add.s64 	%rd18, %rd17, %rd16;
	add.s64 	%rd19, %rd18, %rd1;
	setp.lt.u64	%p2, %rd19, %rd18;
	mov.b64	{%r17, %r18}, %rd19;
	cvt.u64.u32	%rd2, %r8;
	mul.wide.u32 	%rd20, %r18, %r8;
	sub.s64 	%rd21, %rd1, %rd20;
	mov.b64	{%r19, %r20}, %rd21;
	selp.b32	%r21, %r8, 0, %p2;
	sub.s32 	%r22, %r20, %r21;
	mov.b64	%rd22, {%r19, %r22};
	setp.ge.s64	%p3, %rd22, %rd2;
	selp.u32	%r23, 1, 0, %p3;
	shr.s32 	%r24, %r22, 31;
	add.s32 	%r25, %r24, %r18;
	setp.gt.s32	%p4, %r22, -1;
	selp.b32	%r26, 0, %r8, %p4;
	add.s32 	%r27, %r26, %r19;
	neg.s32 	%r28, %r8;
	selp.b32	%r29, %r28, 0, %p3;
	add.s32 	%r30, %r27, %r29;
	add.s32 	%r31, %r25, %r23;
	mov.b64	%rd3, {%r31, %r30};
	and.b64  	%rd23, %rd1, -4294967296;
	setp.eq.s64	%p5, %rd23, 0;
	@%p5 bra 	BB2_2;

	div.u64 	%rd28, %rd1, %rd2;
	rem.u64 	%rd29, %rd1, %rd2;
	bra.uni 	BB2_3;

BB2_2:
	cvt.u32.u64	%r32, %rd2;
	cvt.u32.u64	%r33, %rd1;
	div.u32 	%r34, %r33, %r32;
	rem.u32 	%r35, %r33, %r32;
	cvt.u64.u32	%rd28, %r34;
	cvt.u64.u32	%rd29, %r35;

BB2_3:
	and.b64  	%rd24, %rd28, 4294967295;
	shl.b64 	%rd25, %rd29, 32;
	or.b64  	%rd10, %rd24, %rd25;
	setp.eq.s64	%p6, %rd3, %rd10;
	@%p6 bra 	BB2_6;

	cvta.to.global.u64 	%rd26, %rd11;
	atom.global.add.u32 	%r36, [%rd26], 1;
	setp.ne.s32	%p7, %r36, 0;
	@%p7 bra 	BB2_6;

	st.global.u64 	[%rd26+8], %rd1;
	st.global.u64 	[%rd26+16], %rd2;
	st.global.u64 	[%rd26+24], %rd3;
	st.global.u64 	[%rd26+32], %rd10;

BB2_6:
	ret;
}

	// .globl	_Z12FastDivBenchPKyjPy
.visible .entry _Z12FastDivBenchPKyjPy(
	.param .u64 _Z12FastDivBenchPKyjPy_param_0,
	.param .u32 _Z12FastDivBenchPKyjPy_param_1,
	.param .u64 _Z12FastDivBenchPKyjPy_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd2, [_Z12FastDivBenchPKyjPy_param_0];
	ld.param.u32 	%r4, [_Z12FastDivBenchPKyjPy_param_1];
	ld.param.u64 	%rd1, [_Z12FastDivBenchPKyjPy_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	mov.u32 	%r5, %tid.x;
	mul.wide.u32 	%rd4, %r5, 8;
	add.s64 	%rd5, %rd3, %rd4;
	mov.u32 	%r6, %ctaid.x;
	add.s32 	%r7, %r6, %r4;
	setp.eq.s32	%p1, %r7, -2147483648;
	selp.b32	%r8, -2147483647, %r7, %p1;
	ld.global.u64 	%rd6, [%rd5];
	mov.b64	{%r9, %r10}, %rd6;
	shr.u32 	%r11, %r8, 8;
	add.s32 	%r1, %r11, 1317011456;
	// inline asm
	mov.b32 	%f1, %r1;
	// inline asm
	and.b32  	%r12, %r8, 255;
	cvt.rn.f32.u32	%f6, %r12;
	// inline asm
	rcp.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r2, %f2;
	// inline asm
	add.s32 	%r3, %r2, 536870912;
	// inline asm
	mov.b32 	%f5, %r3;
	// inline asm
	mov.f32 	%f7, 0fBF800000;
	fma.rn.f32 	%f8, %f1, %f2, %f7;
	fma.rn.f32 	%f9, %f6, %f2, %f8;
	shl.b32 	%r13, %r2, 9;
	mul.f32 	%f10, %f5, %f9;
	cvt.rni.s32.f32	%r14, %f10;
	sub.s32 	%r15, %r13, %r14;
	mul.hi.u32 	%r16, %r9, %r15;
	cvt.u64.u32	%rd7, %r16;
	mul.wide.u32 	%rd8, %r15, %r10;
	add.s64 	%rd9, %rd8, %rd7;
	add.s64 	%rd10, %rd9, %rd6;
	setp.lt.u64	%p2, %rd10, %rd9;
	mov.b64	{%r17, %r18}, %rd10;
	cvt.u64.u32	%rd11, %r8;
	mul.wide.u32 	%rd12, %r18, %r8;
	sub.s64 	%rd13, %rd6, %rd12;
	mov.b64	{%r19, %r20}, %rd13;
	selp.b32	%r21, %r8, 0, %p2;
	sub.s32 	%r22, %r20, %r21;
	mov.b64	%rd14, {%r19, %r22};
	setp.ge.s64	%p3, %rd14, %rd11;
	selp.u32	%r23, 1, 0, %p3;
	shr.s32 	%r24, %r22, 31;
	add.s32 	%r25, %r24, %r18;
	setp.gt.s32	%p4, %r22, -1;
	selp.b32	%r26, 0, %r8, %p4;
	add.s32 	%r27, %r26, %r19;
	neg.s32 	%r28, %r8;
	selp.b32	%r29, %r28, 0, %p3;
	add.s32 	%r30, %r27, %r29;
	add.s32 	%r31, %r25, %r23;
	mov.b64	%rd15, {%r31, %r30};
	setp.ne.s64	%p5, %rd15, 1;
	@%p5 bra 	BB3_2;

	cvta.to.global.u64 	%rd16, %rd1;
	mov.u64 	%rd17, 0;
	st.global.u64 	[%rd16], %rd17;

BB3_2:
	ret;
}

	// .globl	_Z12FastSqrtTestjPy
.visible .entry _Z12FastSqrtTestjPy(
	.param .u32 _Z12FastSqrtTestjPy_param_0,
	.param .u64 _Z12FastSqrtTestjPy_param_1
)
{
	.reg .pred 	%p<18>;
	.reg .f32 	%f<51>;
	.reg .b32 	%r<109>;
	.reg .b64 	%rd<99>;


	ld.param.u32 	%r3, [_Z12FastSqrtTestjPy_param_0];
	ld.param.u64 	%rd15, [_Z12FastSqrtTestjPy_param_1];
	cvta.to.global.u64 	%rd1, %rd15;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r4, %r5, %r3;
	mov.u32 	%r7, %tid.x;
	add.s32 	%r1, %r6, %r7;
	setp.gt.s32	%p1, %r1, 1779033702;
	@%p1 bra 	BB4_13;
	bra.uni 	BB4_1;

BB4_13:
	setp.ne.s32	%p14, %r1, 1779033703;
	@%p14 bra 	BB4_17;

	mov.u32 	%r92, 1610612735;
	// inline asm
	mov.b32 	%f41, %r92;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f42, %f41;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f44, %f41;
	// inline asm
	// inline asm
	mov.b32 	%r93, %f42;
	// inline asm
	add.s32 	%r94, %r93, 268435456;
	// inline asm
	mov.b32 	%f47, %r94;
	// inline asm
	// inline asm
	mov.b32 	%r95, %f44;
	// inline asm
	add.s32 	%r96, %r95, -1325400064;
	mul.wide.u32 	%rd80, %r96, %r96;
	shl.b64 	%rd81, %rd80, 18;
	not.b64 	%rd82, %rd81;
	mov.b64	{%r97, %r98}, %rd82;
	cvt.rn.f32.s32	%f49, %r98;
	mul.f32 	%f50, %f47, %f49;
	shl.b32 	%r99, %r96, 10;
	cvt.rni.s32.f32	%r100, %f50;
	add.s32 	%r101, %r99, %r100;
	shr.u32 	%r102, %r101, 1;
	and.b32  	%r103, %r101, 1;
	cvt.u64.u32	%rd83, %r102;
	add.s32 	%r104, %r102, %r103;
	mul.wide.u32 	%rd84, %r104, %r102;
	cvt.u64.u32	%rd85, %r101;
	shl.b64 	%rd86, %rd85, 32;
	add.s64 	%rd87, %rd86, %rd84;
	add.s64 	%rd88, %rd87, 1;
	cvt.u64.u32	%rd89, %r103;
	add.s64 	%rd90, %rd88, %rd89;
	setp.gt.s64	%p15, %rd90, 0;
	selp.b32	%r105, -1, 0, %p15;
	add.s32 	%r106, %r105, %r101;
	add.s64 	%rd91, %rd83, %rd88;
	add.s64 	%rd92, %rd91, 4294967296;
	shr.u64 	%rd93, %rd92, 63;
	cvt.u32.u64	%r107, %rd93;
	add.s32 	%r2, %r106, %r107;
	setp.eq.s32	%p16, %r2, -736899889;
	@%p16 bra 	BB4_17;

	atom.global.add.u32 	%r108, [%rd1], 1;
	setp.ne.s32	%p17, %r108, 0;
	@%p17 bra 	BB4_17;

	cvt.u64.u32	%rd95, %r2;
	mov.u64 	%rd96, 1779033703;
	st.global.u64 	[%rd1+8], %rd96;
	mov.u64 	%rd97, -1;
	st.global.u64 	[%rd1+16], %rd97;
	mov.u64 	%rd98, 3558067407;
	st.global.u64 	[%rd1+24], %rd98;
	st.global.u64 	[%rd1+32], %rd95;
	bra.uni 	BB4_17;

BB4_1:
	cvt.s64.s32	%rd2, %r1;
	add.s64 	%rd3, %rd2, 4294967296;
	mul.lo.s64 	%rd4, %rd3, %rd3;
	mov.b64	{%r12, %r13}, %rd4;
	shr.u32 	%r14, %r13, 9;
	add.s32 	%r8, %r14, 1602224128;
	// inline asm
	mov.b32 	%f1, %r8;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f2, %f1;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f4, %f1;
	// inline asm
	// inline asm
	mov.b32 	%r9, %f2;
	// inline asm
	add.s32 	%r10, %r9, 268435456;
	// inline asm
	mov.b32 	%f7, %r10;
	// inline asm
	// inline asm
	mov.b32 	%r11, %f4;
	// inline asm
	add.s32 	%r15, %r11, -1325400064;
	mul.wide.u32 	%rd16, %r15, %r15;
	shl.b64 	%rd17, %rd16, 18;
	sub.s64 	%rd18, %rd4, %rd17;
	mov.b64	{%r16, %r17}, %rd18;
	cvt.rn.f32.s32	%f9, %r17;
	mul.f32 	%f10, %f7, %f9;
	shl.b32 	%r18, %r15, 10;
	cvt.rni.s32.f32	%r19, %f10;
	add.s32 	%r20, %r18, %r19;
	shr.u32 	%r21, %r20, 1;
	and.b32  	%r22, %r20, 1;
	cvt.u64.u32	%rd19, %r21;
	add.s32 	%r23, %r21, %r22;
	mul.wide.u32 	%rd20, %r23, %r21;
	cvt.u64.u32	%rd21, %r20;
	shl.b64 	%rd22, %rd21, 32;
	sub.s64 	%rd23, %rd22, %rd4;
	add.s64 	%rd24, %rd20, %rd23;
	cvt.u64.u32	%rd25, %r22;
	add.s64 	%rd26, %rd24, %rd25;
	setp.gt.s64	%p2, %rd26, 0;
	selp.b32	%r24, -1, 0, %p2;
	add.s32 	%r25, %r24, %r20;
	add.s64 	%rd27, %rd19, %rd24;
	add.s64 	%rd28, %rd27, 4294967296;
	shr.u64 	%rd29, %rd28, 63;
	cvt.u32.u64	%r26, %rd29;
	add.s32 	%r27, %r25, %r26;
	cvt.u64.u32	%rd5, %r27;
	shl.b64 	%rd6, %rd3, 1;
	add.s64 	%rd7, %rd6, -8589934592;
	setp.eq.s64	%p3, %rd5, %rd7;
	@%p3 bra 	BB4_4;

	atom.global.add.u32 	%r28, [%rd1], 1;
	setp.ne.s32	%p4, %r28, 0;
	@%p4 bra 	BB4_4;

	st.global.u64 	[%rd1+8], %rd2;
	st.global.u64 	[%rd1+16], %rd4;
	st.global.u64 	[%rd1+24], %rd7;
	st.global.u64 	[%rd1+32], %rd5;

BB4_4:
	add.s64 	%rd8, %rd4, %rd3;
	mov.b64	{%r33, %r34}, %rd8;
	shr.u32 	%r35, %r34, 9;
	add.s32 	%r29, %r35, 1602224128;
	// inline asm
	mov.b32 	%f11, %r29;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f12, %f11;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f14, %f11;
	// inline asm
	// inline asm
	mov.b32 	%r30, %f12;
	// inline asm
	add.s32 	%r31, %r30, 268435456;
	// inline asm
	mov.b32 	%f17, %r31;
	// inline asm
	// inline asm
	mov.b32 	%r32, %f14;
	// inline asm
	add.s32 	%r36, %r32, -1325400064;
	mul.wide.u32 	%rd31, %r36, %r36;
	shl.b64 	%rd32, %rd31, 18;
	sub.s64 	%rd33, %rd8, %rd32;
	mov.b64	{%r37, %r38}, %rd33;
	cvt.rn.f32.s32	%f19, %r38;
	mul.f32 	%f20, %f17, %f19;
	shl.b32 	%r39, %r36, 10;
	cvt.rni.s32.f32	%r40, %f20;
	add.s32 	%r41, %r39, %r40;
	shr.u32 	%r42, %r41, 1;
	and.b32  	%r43, %r41, 1;
	cvt.u64.u32	%rd34, %r42;
	add.s32 	%r44, %r42, %r43;
	mul.wide.u32 	%rd35, %r44, %r42;
	cvt.u64.u32	%rd36, %r41;
	shl.b64 	%rd37, %rd36, 32;
	sub.s64 	%rd38, %rd37, %rd8;
	add.s64 	%rd39, %rd35, %rd38;
	cvt.u64.u32	%rd40, %r43;
	add.s64 	%rd41, %rd39, %rd40;
	setp.gt.s64	%p5, %rd41, 0;
	selp.b32	%r45, -1, 0, %p5;
	add.s32 	%r46, %r45, %r41;
	add.s64 	%rd42, %rd34, %rd39;
	add.s64 	%rd43, %rd42, 4294967296;
	shr.u64 	%rd44, %rd43, 63;
	cvt.u32.u64	%r47, %rd44;
	add.s32 	%r48, %r46, %r47;
	cvt.u64.u32	%rd9, %r48;
	setp.eq.s64	%p6, %rd9, %rd7;
	@%p6 bra 	BB4_7;

	atom.global.add.u32 	%r49, [%rd1], 1;
	setp.ne.s32	%p7, %r49, 0;
	@%p7 bra 	BB4_7;

	st.global.u64 	[%rd1+8], %rd2;
	st.global.u64 	[%rd1+16], %rd8;
	st.global.u64 	[%rd1+24], %rd7;
	st.global.u64 	[%rd1+32], %rd9;

BB4_7:
	add.s64 	%rd10, %rd8, 1;
	mov.b64	{%r54, %r55}, %rd10;
	shr.u32 	%r56, %r55, 9;
	add.s32 	%r50, %r56, 1602224128;
	// inline asm
	mov.b32 	%f21, %r50;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f22, %f21;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f24, %f21;
	// inline asm
	// inline asm
	mov.b32 	%r51, %f22;
	// inline asm
	add.s32 	%r52, %r51, 268435456;
	// inline asm
	mov.b32 	%f27, %r52;
	// inline asm
	// inline asm
	mov.b32 	%r53, %f24;
	// inline asm
	add.s32 	%r57, %r53, -1325400064;
	mul.wide.u32 	%rd46, %r57, %r57;
	shl.b64 	%rd47, %rd46, 18;
	sub.s64 	%rd48, %rd10, %rd47;
	mov.b64	{%r58, %r59}, %rd48;
	cvt.rn.f32.s32	%f29, %r59;
	mul.f32 	%f30, %f27, %f29;
	shl.b32 	%r60, %r57, 10;
	cvt.rni.s32.f32	%r61, %f30;
	add.s32 	%r62, %r60, %r61;
	shr.u32 	%r63, %r62, 1;
	and.b32  	%r64, %r62, 1;
	cvt.u64.u32	%rd49, %r63;
	add.s32 	%r65, %r63, %r64;
	mul.wide.u32 	%rd50, %r65, %r63;
	cvt.u64.u32	%rd51, %r62;
	shl.b64 	%rd52, %rd51, 32;
	sub.s64 	%rd53, %rd52, %rd10;
	add.s64 	%rd54, %rd50, %rd53;
	cvt.u64.u32	%rd55, %r64;
	add.s64 	%rd56, %rd54, %rd55;
	setp.gt.s64	%p8, %rd56, 0;
	selp.b32	%r66, -1, 0, %p8;
	add.s32 	%r67, %r66, %r62;
	add.s64 	%rd57, %rd49, %rd54;
	add.s64 	%rd58, %rd57, 4294967296;
	shr.u64 	%rd59, %rd58, 63;
	cvt.u32.u64	%r68, %rd59;
	add.s32 	%r69, %r67, %r68;
	cvt.u64.u32	%rd60, %r69;
	add.s64 	%rd11, %rd6, -8589934591;
	setp.eq.s64	%p9, %rd60, %rd11;
	@%p9 bra 	BB4_10;

	atom.global.add.u32 	%r70, [%rd1], 1;
	setp.ne.s32	%p10, %r70, 0;
	@%p10 bra 	BB4_10;

	st.global.u64 	[%rd1+8], %rd2;
	st.global.u64 	[%rd1+16], %rd10;
	st.global.u64 	[%rd1+24], %rd11;
	st.global.u64 	[%rd1+32], %rd9;

BB4_10:
	add.s64 	%rd62, %rd2, 4294967297;
	mul.lo.s64 	%rd63, %rd62, %rd62;
	add.s64 	%rd12, %rd63, -1;
	mov.b64	{%r75, %r76}, %rd12;
	shr.u32 	%r77, %r76, 9;
	add.s32 	%r71, %r77, 1602224128;
	// inline asm
	mov.b32 	%f31, %r71;
	// inline asm
	// inline asm
	rsqrt.approx.f32 %f32, %f31;
	// inline asm
	// inline asm
	sqrt.approx.f32 %f34, %f31;
	// inline asm
	// inline asm
	mov.b32 	%r72, %f32;
	// inline asm
	add.s32 	%r73, %r72, 268435456;
	// inline asm
	mov.b32 	%f37, %r73;
	// inline asm
	// inline asm
	mov.b32 	%r74, %f34;
	// inline asm
	add.s32 	%r78, %r74, -1325400064;
	mul.wide.u32 	%rd64, %r78, %r78;
	shl.b64 	%rd65, %rd64, 18;
	sub.s64 	%rd66, %rd12, %rd65;
	mov.b64	{%r79, %r80}, %rd66;
	cvt.rn.f32.s32	%f39, %r80;
	mul.f32 	%f40, %f37, %f39;
	shl.b32 	%r81, %r78, 10;
	cvt.rni.s32.f32	%r82, %f40;
	add.s32 	%r83, %r81, %r82;
	shr.u32 	%r84, %r83, 1;
	and.b32  	%r85, %r83, 1;
	cvt.u64.u32	%rd67, %r84;
	add.s32 	%r86, %r84, %r85;
	mul.wide.u32 	%rd68, %r86, %r84;
	cvt.u64.u32	%rd69, %r83;
	shl.b64 	%rd70, %rd69, 32;
	sub.s64 	%rd71, %rd70, %rd12;
	add.s64 	%rd72, %rd68, %rd71;
	cvt.u64.u32	%rd73, %r85;
	add.s64 	%rd74, %rd72, %rd73;
	setp.gt.s64	%p11, %rd74, 0;
	selp.b32	%r87, -1, 0, %p11;
	add.s32 	%r88, %r87, %r83;
	add.s64 	%rd75, %rd67, %rd72;
	add.s64 	%rd76, %rd75, 4294967296;
	shr.u64 	%rd77, %rd76, 63;
	cvt.u32.u64	%r89, %rd77;
	add.s32 	%r90, %r88, %r89;
	cvt.u64.u32	%rd13, %r90;
	shl.b64 	%rd78, %rd62, 1;
	add.s64 	%rd14, %rd78, -8589934593;
	setp.eq.s64	%p12, %rd13, %rd14;
	@%p12 bra 	BB4_17;

	atom.global.add.u32 	%r91, [%rd1], 1;
	setp.ne.s32	%p13, %r91, 0;
	@%p13 bra 	BB4_17;

	st.global.u64 	[%rd1+8], %rd2;
	st.global.u64 	[%rd1+16], %rd12;
	st.global.u64 	[%rd1+24], %rd14;
	st.global.u64 	[%rd1+32], %rd13;

BB4_17:
	ret;
}


